{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b62fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for BLIP2 multi-head regression\n",
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers accelerate sentencepiece\n",
    "!pip install pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e083fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MOVEMENT_DIM = 5\n",
    "GENRE_DIM = 5\n",
    "STYLE_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c653d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get the images\n",
    "from PIL import Image\n",
    "import os, json, torch\n",
    "\n",
    "\n",
    "try:  # Local path to images\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "    mount_path = '/content/drive'\n",
    "    # Mount only if not already mounted\n",
    "    if not os.path.exists(mount_path):\n",
    "        drive.mount(mount_path)\n",
    "    imgs_directory_path = '/content/drive/MyDrive/ArtEmbed'\n",
    "    pretraining_metadata = '/content/drive/MyDrive/ArtEmbed/wikiart_metadata_with_pretraining_groundtruth.json'\n",
    "\n",
    "except:\n",
    "    from pathlib import Path\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Google Colab\")\n",
    "\n",
    "    BASE_DIR = Path(__file__).resolve().parent  # folder where embed_model.py lives\n",
    "    imgs_directory_path = BASE_DIR / \"paintings\"\n",
    "    pretraining_metadata = BASE_DIR / \"metadata\" / \"wikiart_metadata_with_pretraining_groundtruth.json\"\n",
    "\n",
    "\n",
    "def load_image_from_drive():\n",
    "  image_array = []\n",
    "  image_names = []\n",
    "  image_ids =[]\n",
    "\n",
    "  all_files = sorted(os.listdir(imgs_directory_path))\n",
    "  for file_name in all_files:\n",
    "      if file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "          path = os.path.join(imgs_directory_path, file_name)\n",
    "          img = Image.open(path).convert(\"RGB\")\n",
    "          image_array.append(img)\n",
    "          image_names.append(file_name)\n",
    "          image_ids.append(file_name.split(\"_\")[0])\n",
    "\n",
    "  print(f\"Found {len(image_array)} images. Image ids: {image_ids}\")\n",
    "  return image_array, image_ids\n",
    "\n",
    "def load_pretraining_metadata():\n",
    "    with open(pretraining_metadata, 'r', encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    # print(metadata.keys())\n",
    "    print(f\"Found metadata for {len(metadata)} paintings.\")\n",
    "    return metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93d5ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Install dependencies\n",
    "!pip install transformers accelerate sentencepiece\n",
    "\n",
    "# --- Import libraries ---\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# --- Load BLIP-2 model and processor ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Salesforce/blip2-flan-t5-xl\"\n",
    "local_model_path =  BASE_DIR / \"blip2_model\"\n",
    "\n",
    "if os.path.exists(local_model_path):\n",
    "    print(\"Loading model from local directory...\")\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    print(\"Processor loaded\")\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(local_model_path)\n",
    "    print(\"Model loaded\")\n",
    "else:\n",
    "    print(\"Downloading model from Hugging Face...\")\n",
    "    processor = Blip2Processor.from_pretrained(model_name, use_fast=True)\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Save to local directory for future use\n",
    "    processor.save_pretrained(local_model_path)\n",
    "    blip2.save_pretrained(local_model_path)\n",
    "\n",
    "blip2.to(\"cpu\")  # Load model on CPU to avoid GPU memory issues\n",
    "print(f\"Loaded model on cpu\")\n",
    "\n",
    "# Freeze vision encoder to save memory; we are not training the vision encoder\n",
    "for param in blip2.vision_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95070a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_dataloader(image_list, target_list, processor, device, batch_size=4, shuffle=True):\n",
    "    # Convert images to pixel values tensors\n",
    "    pixel_values_tensor = torch.stack([\n",
    "        processor(images=img, return_tensors=\"pt\").pixel_values.squeeze(0) \n",
    "        for img in image_list\n",
    "    ])  # [N, 3, H, W]\n",
    "\n",
    "    # Convert targets to tensor\n",
    "    targets_tensor = torch.stack([torch.tensor(t, dtype=torch.float32) for t in target_list])  # [N, total_dims]\n",
    "\n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(pixel_values_tensor, targets_tensor)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Wrap batches with device transfer\n",
    "    def device_loader():\n",
    "        for batch_pixel_values, batch_targets in dataloader:\n",
    "            yield batch_pixel_values.to(device, non_blocking=True), batch_targets.to(device, non_blocking=True)\n",
    "\n",
    "    print(f\"Created DataLoader with {len(dataloader)} batches of size {batch_size}\")\n",
    "\n",
    "    return device_loader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be0459",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_gpu_mem(prefix=\"GPU\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2   # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2     # MB\n",
    "        print(f\"{prefix} Memory — Allocated: {allocated:.2f} MB | Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e21e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BLIP2MultiHeadRegression(nn.Module):\n",
    "    def __init__(self, blip2_model,\n",
    "                 use_style_head=True,\n",
    "                 train_qformer=False,\n",
    "                 train_vision=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Devices ---\n",
    "        self.main_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.vision_device = torch.device(\"cpu\")  # keep heavy vision encoder on CPU\n",
    "\n",
    "        # --- Core model ---\n",
    "        self.blip2 = blip2_model\n",
    "        self.use_style_head = use_style_head\n",
    "\n",
    "        # --- Control what's trainable ---\n",
    "        for param in self.blip2.vision_model.parameters():\n",
    "            param.requires_grad = train_vision\n",
    "        for param in self.blip2.qformer.parameters():\n",
    "            param.requires_grad = train_qformer\n",
    "\n",
    "        # --- Move modules to appropriate devices ---\n",
    "        self.blip2.vision_model.to(self.vision_device)\n",
    "        self.blip2.qformer.to(self.main_device)\n",
    "\n",
    "        # query_tokens is an nn.Parameter → rewrap properly after moving\n",
    "        self.blip2.query_tokens = nn.Parameter(\n",
    "            self.blip2.query_tokens.to(self.main_device)\n",
    "        )\n",
    "\n",
    "        # --- Config info ---\n",
    "        num_query_tokens = blip2_model.config.num_query_tokens\n",
    "        hidden_size = blip2_model.config.qformer_config.hidden_size\n",
    "        feature_dim = num_query_tokens * hidden_size\n",
    "\n",
    "        print(f\"Num query tokens: {num_query_tokens}\")\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        print(f\"Feature dim: {feature_dim}\")\n",
    "        print(f\"Use style head: {use_style_head}\")\n",
    "        print(f\"Vision model device: {self.vision_device}\")\n",
    "        print(f\"Q-Former device: {self.main_device}\")\n",
    "\n",
    "        # --- Shared feature extraction ---\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "        # --- Movement and Genre heads ---\n",
    "        self.movement_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, MOVEMENT_DIM)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "        self.genre_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, GENRE_DIM)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "        # --- Style head (always defined, but only used if enabled) ---\n",
    "        self.style_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, STYLE_DIM)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "    def forward(self, images, return_features=False):\n",
    "        \"\"\"\n",
    "        Forward pass with optional CPU/GPU split for vision model.\n",
    "\n",
    "        Args:\n",
    "            images: [batch_size, 3, H, W]\n",
    "            return_features: If True, also return shared features\n",
    "\n",
    "        Returns:\n",
    "            dict with keys: 'movement', 'genre', 'style', 'combined', optionally 'features'\n",
    "        \"\"\"\n",
    "        device = next(self.shared_features.parameters()).device  # GPU for rest of model\n",
    "\n",
    "        # --- Vision encoding ---\n",
    "        vision_device = next(self.blip2.vision_model.parameters()).device\n",
    "        images_vision = images.to(vision_device)\n",
    "\n",
    "        if self.training and next(self.blip2.vision_model.parameters()).requires_grad:\n",
    "            vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "\n",
    "        image_embeds = vision_outputs.last_hidden_state.to(device)  # move to GPU\n",
    "\n",
    "        # --- Q-Former processing ---\n",
    "        query_tokens = self.blip2.query_tokens.expand(images.shape[0], -1, -1).to(device)\n",
    "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "        query_outputs = self.blip2.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # --- Flatten Q-Former output ---\n",
    "        query_hidden_states = query_outputs.last_hidden_state\n",
    "        flattened = query_hidden_states.flatten(start_dim=1)\n",
    "\n",
    "        # --- Shared features ---\n",
    "        shared_features = self.shared_features(flattened)\n",
    "\n",
    "        # --- Regression heads ---\n",
    "        movement_scores = torch.sigmoid(self.movement_head(shared_features))\n",
    "        genre_scores = torch.sigmoid(self.genre_head(shared_features))\n",
    "        style_scores = torch.sigmoid(self.style_head(shared_features))\n",
    "\n",
    "        outputs = {\n",
    "            'movement': movement_scores,\n",
    "            'genre': genre_scores,\n",
    "            'style': style_scores,\n",
    "            'combined': torch.cat([movement_scores, genre_scores, style_scores], dim=1)\n",
    "        }\n",
    "\n",
    "        if return_features:\n",
    "            outputs['features'] = shared_features\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class WeightedMultiHeadLoss(nn.Module):\n",
    "    def __init__(self, movement_weight=1.0, genre_weight=0.7, style_weight=0.8, use_style=True):\n",
    "        super().__init__()\n",
    "        self.movement_weight = movement_weight\n",
    "        self.genre_weight = genre_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.use_style = use_style\n",
    "\n",
    "    def forward(self, predictions, targets, confidences=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: dict with 'movement', 'genre', 'style'\n",
    "            targets: tensor [batch, total_dim] (already prepared)\n",
    "            confidences: dict with confidence scores (optional)\n",
    "        \"\"\"\n",
    "        # Split targets using global dims\n",
    "        movement_target = targets[:, :MOVEMENT_DIM]\n",
    "        genre_target    = targets[:, MOVEMENT_DIM : MOVEMENT_DIM + GENRE_DIM]\n",
    "        style_target    = targets[:, MOVEMENT_DIM + GENRE_DIM :]\n",
    "\n",
    "        mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "        # Movement loss\n",
    "        movement_loss = mse(predictions['movement'], movement_target)\n",
    "        if confidences is not None and 'movement' in confidences:\n",
    "            movement_loss = movement_loss * confidences['movement']\n",
    "        movement_loss = movement_loss.mean() * self.movement_weight\n",
    "\n",
    "        # Genre loss\n",
    "        genre_loss = mse(predictions['genre'], genre_target)\n",
    "        if confidences is not None and 'genre' in confidences:\n",
    "            genre_loss = genre_loss * confidences['genre']\n",
    "        genre_loss = genre_loss.mean() * self.genre_weight\n",
    "\n",
    "        total_loss = movement_loss + genre_loss\n",
    "        loss_dict = {'movement': movement_loss.item(), 'genre': genre_loss.item()}\n",
    "\n",
    "        # Style loss\n",
    "        if self.use_style:\n",
    "            style_loss = mse(predictions['style'], style_target)\n",
    "            if confidences is not None and 'style' in confidences:\n",
    "                style_loss = style_loss * confidences['style']\n",
    "            style_loss = style_loss.mean() * self.style_weight\n",
    "            total_loss += style_loss\n",
    "            loss_dict['style'] = style_loss.item()\n",
    "\n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033942c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    num_images = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(dataloader):\n",
    "        batch_size = pixel_values.size(0)\n",
    "        num_batches += 1\n",
    "        num_images += batch_size\n",
    "\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)  # [batch, total_dim]\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values)\n",
    "        loss, loss_dict = criterion(predictions, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        time_elapsed = time.time() - start_time\n",
    "        # if step % 2 == 0:\n",
    "        if True:\n",
    "            print(f\"Step {step} image number {num_images} time_elapsed {time_elapsed:.2f}s | Loss: {loss.item():.4f}\")\n",
    "            print_gpu_mem()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch complete | Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Time: {epoch_time:.2f}s | Per batch: {epoch_time/num_batches:.2f}s | Per image: {epoch_time/num_images:.4f}s\")\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def test_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pixel_values, targets in dataloader:\n",
    "            pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)  # [batch, total_dim]\n",
    "\n",
    "            predictions = model(pixel_values)\n",
    "            loss, _ = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation complete | Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7d3d5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10, save_path=None):\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            val_loss = test_epoch(model, val_loader, criterion, device)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if save_path is not None:\n",
    "            checkpoint_file = f\"{save_path}/model_epoch_{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), checkpoint_file)\n",
    "            print(f\"Saved checkpoint: {checkpoint_file}\")\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0792e91",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "def split_train_test(image_list, targets, test_percentage=0.1):\n",
    "    test_percentage = 0.1\n",
    "    num_images = len(image_list)\n",
    "    num_test = int(num_images * test_percentage)\n",
    "\n",
    "    # Randomly sample indices for test set\n",
    "    test_indices = random.sample(range(num_images), num_test)\n",
    "\n",
    "    # Create test images and targets\n",
    "    test_images = [image_list[i] for i in test_indices]\n",
    "    test_targets = [targets[i] for i in test_indices]\n",
    "\n",
    "    # Optionally, remove test items from the training set\n",
    "    train_images = [img for idx, img in enumerate(image_list) if idx not in test_indices]\n",
    "    train_targets = [tgt for idx, tgt in enumerate(targets) if idx not in test_indices]\n",
    "\n",
    "    train_loader = create_dataloader(\n",
    "        train_images, train_targets, processor, device, batch_size=16, shuffle=True\n",
    "    )\n",
    "    test_loader = create_dataloader(\n",
    "        test_images, test_targets, processor, device, batch_size=32, shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5785530",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pretrain_model():\n",
    "    # PRETRAINING: No style head\n",
    "    print(\"=\"*50)\n",
    "    print(\"PRETRAINING MODE (no style head)\")\n",
    "    print(\"=\"*50)\n",
    "    image_list, image_ids = load_image_from_drive()\n",
    "    pretraining_metadata = load_pretraining_metadata()\n",
    "    from augmentation import augment_images_for_pretraining\n",
    "    image_list, image_ids, targets = augment_images_for_pretraining(image_list, image_ids, pretraining_metadata)   \n",
    "\n",
    "\n",
    "    pretrain_model = BLIP2MultiHeadRegression( blip2,\n",
    "        use_style_head=False, train_qformer=False, train_vision=False\n",
    "    )\n",
    "    pretrain_criterion = WeightedMultiHeadLoss( movement_weight=1.0, genre_weight=0.7,\n",
    "        use_style=False,).to(device)\n",
    "    optimizer = torch.optim.AdamW(pretrain_model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    train_loader, test_loader = split_train_test(image_list, targets, test_percentage=0.1)\n",
    "    save_dir = \"./checkpoints\"\n",
    "    history = train_model(pretrain_model, train_loader, test_loader, optimizer, pretrain_criterion,\n",
    "        device, num_epochs=1, save_path=save_dir\n",
    "    )\n",
    "# pretrain_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019ab0a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from transformers import Blip2Processor\n",
    "from augmentation import augment_annotated_images\n",
    "\n",
    "# --- Global variables for lazy loading ---\n",
    "_model, _processor = None, None\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir=\"./checkpoints\"):\n",
    "    \"\"\"Return the latest checkpoint path or None if none exist.\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"model_epoch_*.pt\"))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    # Sort by epoch number\n",
    "    checkpoint_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split(\"_\")[-1]))\n",
    "    return checkpoint_files[-1]\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "BASE_DIR = \"/path/to/your/project\"  # replace with your BASE_DIR\n",
    "\n",
    "def save_model_checkpoint(model):\n",
    "    checkpoint_dir = os.path.join(BASE_DIR, \".checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # Generate abbreviated timestamp (YYMMDD_HHMMSS)\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "    # Build checkpoint path\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_{timestamp}.pt\")\n",
    "\n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "def initialize_model_for_webaccess():\n",
    "    \"\"\"\n",
    "    Initialize the BLIP2 multi-head regression model and processor.\n",
    "    Loads the latest checkpoint if available.\n",
    "    \"\"\"\n",
    "    model = BLIP2MultiHeadRegression(\n",
    "        blip2,\n",
    "        use_style_head=True,\n",
    "        train_qformer=True,\n",
    "        train_vision=False\n",
    "    )\n",
    "\n",
    "    latest_ckpt = get_latest_checkpoint()\n",
    "    if latest_ckpt is not None:\n",
    "        model.load_state_dict(torch.load(latest_ckpt, map_location=\"cpu\"))\n",
    "        print(f\"Loaded model weights from {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found, using untrained weights.\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    return model, processor\n",
    "\n",
    "def get_model_and_processor():\n",
    "    \"\"\"\n",
    "    Lazy-load the model and processor.\n",
    "    \"\"\"\n",
    "    global _model, _processor\n",
    "    if _model is None or _processor is None:\n",
    "        _model, _processor = initialize_model_for_webaccess()\n",
    "        print(f\"Model and processor ready\")\n",
    "    return _model, _processor\n",
    "\n",
    "def forward_images(images):\n",
    "    model, processor = get_model_and_processor()\n",
    "    model.eval()\n",
    "\n",
    "    # Process all images as a batch\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    embeddings = outputs[\"combined\"].cpu().tolist()\n",
    "    print(f\"Forward pass completed on {len(images)} images\")\n",
    "    return embeddings\n",
    "\n",
    "def backward_single_image(image, target, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Perform a single training step on one image.\n",
    "    \"\"\"\n",
    "    model, processor = get_model_and_processor()\n",
    "    criterion = WeightedMultiHeadLoss(movement_weight=1.0, genre_weight=1.0, use_style=True).to(device)\n",
    "\n",
    "    augmented_images, augmented_targets = augment_annotated_images([image], [target])\n",
    "    print(f\"Augmented to {len(augmented_images)} images for training\")\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "    inputs = processor(images=augmented_images, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    target_tensor = torch.tensor(augmented_targets, dtype=torch.float32).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    print(\"Model outputs obtained\", outputs.keys())\n",
    "    loss, loss_dict = criterion(outputs, target_tensor)\n",
    "\n",
    "    print(\"Backward pass with loss:\", loss.item(), loss_dict)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    return loss.item(), loss_dict"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "IDAS Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
