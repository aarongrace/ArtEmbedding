# -*- coding: utf-8 -*-
"""BLIP-2 gen text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19CQr-0SllBrBo7L2cexKs_7aj_-ZCZKv
"""

image_array = []
targets = []

# get the images
import numpy as np
from PIL import Image
import os
from google.colab import drive
import torch

def load_image_from_drive():
  mount_path = '/content/drive'
  # Mount only if not already mounted
  if not os.path.exists(mount_path):
      drive.mount(mount_path)

  directory_path = '/content/drive/MyDrive/ArtEmbed'
  image_arrays = []
  image_names = []

  all_files = sorted(os.listdir(directory_path))
  for file_name in all_files:
      if file_name.lower().endswith((".jpg", ".jpeg", ".png")):
          path = os.path.join(directory_path, file_name)
          img = Image.open(path).convert("RGB")
          image_arrays.append(img)
          image_names.append(file_name)

  print(f"Found {len(image_arrays)} images.")
  print(image_names)
  return image_arrays

image_arrays = load_image_from_drive()



targets = torch.tensor([
    [1.0, 0.0],  # painting 1: fully Romantic
    [0.0, 1.0],  # painting 2: fully Realist
])

# Install dependencies
# !pip install transformers accelerate sentencepiece

# --- Import libraries ---
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import requests
import torch

# --- Load BLIP-2 model and processor ---
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "Salesforce/blip2-flan-t5-xl"
processor = Blip2Processor.from_pretrained(model_name)
blip2 = Blip2ForConditionalGeneration.from_pretrained(model_name)

pixel_values_list = [processor(images=img, return_tensors="pt").pixel_values for img in image_arrays]
pixel_values = torch.cat(pixel_values_list, dim=0)  # shape: [N, 3, H, W]

# Freeze vision encoder to save memory; we are not training the vision encoder
for param in blip2.vision_model.parameters():
    param.requires_grad = False

from torch import nn
class BLIP2Regression(nn.Module):
    def __init__(self, blip2_model, output_dim=16, train_qformer=False):
        super().__init__()
        self.blip2 = blip2_model
        # # Optionally train Q-Former
        # for param in self.blip2.qformer.parameters():
        #     param.requires_grad = train_qformer

        num_query_tokens = blip2.config.num_query_tokens
        hidden_size = blip2.config.qformer_config.hidden_size
        print(f"Num query tokens: {num_query_tokens}")
        print(f"Hidden size: {hidden_size}")

        self.regressor = nn.Sequential(
            nn.Linear(num_query_tokens * hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim)
        )
        # start 1024
        # three hidden layers


    def forward(self, images):
        print(f"\n=== Forward Pass Debug ===")
        print(f"Input images shape: {images.shape}")

        # Get vision features
        with torch.no_grad():
            vision_outputs = self.blip2.vision_model(pixel_values=images)
            image_embeds = vision_outputs.last_hidden_state
            #Image embeds shape: torch.Size([1, 257, 1408])

        print(f"Image embeds shape: {image_embeds.shape}")

        device = images.device  # Fixed: use images.device, not pixel_values.device
        query_tokens = self.blip2.query_tokens.expand(images.shape[0], -1, -1)
        query_tokens = query_tokens.to(device)
        # Query tokens shape: torch.Size([1, 32, 768])

        print(f"Query tokens shape: {query_tokens.shape}")

        image_attention_mask = torch.ones(
            image_embeds.shape[:-1], dtype=torch.long
        ).to(device)

        print(f"Image attention mask shape: {image_attention_mask.shape}")
        # Image attention mask shape: torch.Size([1, 257])

        query_outputs = self.blip2.qformer(
            query_embeds=query_tokens,
            encoder_hidden_states=image_embeds,
            encoder_attention_mask=image_attention_mask,
            return_dict=True,
        )

        print(f"Query outputs keys: {query_outputs.keys()}")
        # Query outputs keys: odict_keys(['last_hidden_state', 'pooler_output'])

        query_hidden_states = query_outputs.last_hidden_state
        print(f"Query hidden states shape: {query_hidden_states.shape}")
        # Query hidden states shape: torch.Size([1, 32, 768])

        flattened = query_hidden_states.flatten(start_dim=1)
        print(f"Flattened shape: {flattened.shape}")
        # Flattened shape: torch.Size([1, 24576])

        output = self.regressor(flattened)
        print(f"Final output shape: {output.shape}")
        print(f"Output values: {output}")
        # Final output shape: torch.Size([1, 2])

        normalized = torch.sigmoid(output)
        print(f"Normalized output values: {normalized}")

        return normalized

output_dim = 2
regression_model = BLIP2Regression(blip2, output_dim=output_dim, train_qformer=False)


single_output = regression_model(pixel_values[0:1])
print("Predicted vector for first image:", single_output)

def train_step(model, pixel_values, targets, optimizer, criterion):
    model.train()
    optimizer.zero_grad()
    predictions = model(pixel_values)
    loss = criterion(predictions, targets)
    loss.backward()
    optimizer.step()
    print(f"Loss: {loss.item()}")
    return loss.item(), predictions

def test_step(model, pixel_values, targets, criterion):
    model.eval()
    with torch.no_grad():
        predictions = model(pixel_values)
        loss = criterion(predictions, targets)
    return loss.item(), predictions

from torch import nn, optim
regression_model = BLIP2Regression(blip2, output_dim=output_dim, train_qformer=False)
criterion = nn.MSELoss()
optimizer = optim.Adam(regression_model.parameters(), lr=1e-4)

num_epochs = 5
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = regression_model(pixel_values)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}")


single_output = regression_model(pixel_values[0:1])
print("Predicted vector for first image:", single_output)