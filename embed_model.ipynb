{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e083fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "MOVEMENT_DIM = 6\n",
    "GENRE_DIM = 6\n",
    "STYLE_DIM = 6\n",
    "PRETRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c653d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running from IDAS, probably\n"
     ]
    }
   ],
   "source": [
    "# get the images\n",
    "from PIL import Image, ImageOps\n",
    "import os, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "MAIN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VISION_DEVICE = MAIN_DEVICE\n",
    "try:  # Check if running in Colab\n",
    "    BASE_DIR = Path(__file__).resolve().parent  # works in scripts\n",
    "    print(\"running from laptop, probably\")\n",
    "    VISION_DEVICE = \"cpu\" # not even GPU mem on laptop\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()  # fallback for notebooks\n",
    "    print(\"running from IDAS, probably\")\n",
    "\n",
    "imgs_directory_path = BASE_DIR / \"paintings\"\n",
    "pretraining_metadata = BASE_DIR / \"metadata\" / \"paintings_metadata_with_rough_groundtruth.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec59fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir=os.path.join(BASE_DIR, \"checkpoints\")):\n",
    "    \"\"\"Return the latest checkpoint path by modified time, or None if none exist.\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"model_*.pt\"))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    # Sort by modification time\n",
    "    checkpoint_files.sort(key=os.path.getmtime)\n",
    "    return checkpoint_files[-1]\n",
    "\n",
    "def load_model_from_latest(model):\n",
    "    \"\"\"\n",
    "    Load state dict into model (for the custom nested format).\n",
    "    \n",
    "    Args:\n",
    "        model: BLIP2MultiHeadRegression model instance\n",
    "        state_dict_path: Path to .pt file with state dict\n",
    "    \"\"\"\n",
    "    latest_check_point = get_latest_checkpoint()\n",
    "    if latest_check_point is None:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return\n",
    "    state_dict = torch.load(latest_check_point, map_location='cpu')\n",
    "    \n",
    "    model.shared_features.load_state_dict(state_dict[\"shared_features\"])\n",
    "    model.movement_head.load_state_dict(state_dict[\"movement_head\"])\n",
    "    model.genre_head.load_state_dict(state_dict[\"genre_head\"])\n",
    "    model.style_head.load_state_dict(state_dict[\"style_head\"])\n",
    "    \n",
    "    # Load Q-Former if it was saved\n",
    "    if \"qformer\" in state_dict:\n",
    "        model.blip2.qformer.load_state_dict(state_dict[\"qformer\"])\n",
    "        print(\"✓ Loaded Q-Former weights\")\n",
    "    \n",
    "    print(f\"✓ Loaded weights from {latest_check_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac93d5ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory...\n",
      "Processor loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1253f329bba423eba6682913948efa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "model sent to cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Import libraries ---\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# --- Load BLIP-2 model and processor ---\n",
    "model_name = \"Salesforce/blip2-flan-t5-xl\"\n",
    "local_model_path =  BASE_DIR / \"blip2_model\"\n",
    "\n",
    "if os.path.exists(local_model_path):\n",
    "    print(\"Loading model from local directory...\")\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    print(\"Processor loaded\")\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(local_model_path)\n",
    "    print(\"Model loaded\")\n",
    "else:\n",
    "    print(\"Downloading model from Hugging Face...\")\n",
    "    processor = Blip2Processor.from_pretrained(model_name, use_fast=True)\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Save to local directory for future use\n",
    "    processor.save_pretrained(local_model_path)\n",
    "    blip2.save_pretrained(local_model_path)\n",
    "\n",
    "blip2.to(VISION_DEVICE)  # Load model on CPU first if on computer\n",
    "print(f\"model sent to {VISION_DEVICE}\")\n",
    "\n",
    "# Freeze vision encoder to save memory; we are not training the vision encoder\n",
    "for param in blip2.vision_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a494bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATASET\n",
      "================================================================================\n",
      "Found 30922 image files in c:\\proggers\\ArtEmbedding\\paintings\n",
      "Loaded metadata for 29995 paintings\n",
      "Matched 29995/30922 images with valid targets\n",
      "Target dimension: 12\n",
      "\n",
      "Train: 26996 images\n",
      "Test: 2999 images\n",
      "Sample train path: c:\\proggers\\ArtEmbedding\\paintings\\000001_paul-bril—a-wooded-landscape-with-a-bridge-and-sportsmen-at-the-edge-of-the-river-1590.jpg\n",
      "Sample train target: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "================================================================================\n",
      "VERIFYING DATALOADER OUTPUT\n",
      "================================================================================\n",
      "\n",
      "Batch 0:\n",
      "  image_paths: 32 items (type: <class 'list'>)\n",
      "  targets: torch.Size([32, 12]) (type: <class 'torch.Tensor'>)\n",
      "  targets dtype: torch.float32\n",
      "  targets range: [0.00, 1.00]\n",
      "  First path: c:\\proggers\\ArtEmbedding\\paintings\\013999_richard-dadd—dr-william-orange-1875.jpg\n",
      "  First target: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "\n",
      "Batch 1:\n",
      "  image_paths: 32 items (type: <class 'list'>)\n",
      "  targets: torch.Size([32, 12]) (type: <class 'torch.Tensor'>)\n",
      "  targets dtype: torch.float32\n",
      "  targets range: [0.00, 1.00]\n",
      "  First path: c:\\proggers\\ArtEmbedding\\paintings\\022756_john-singer-sargent—portrait-of-katharine-chase-shapleigh-1890.jpg\n",
      "  First target: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
      "\n",
      "✓ DataLoader verification complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class PaintingDataset(Dataset):\n",
    "    \"\"\"Proper PyTorch Dataset for painting images and targets\"\"\"\n",
    "    def __init__(self, image_paths, targets):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        target = self.targets[idx]\n",
    "        # Convert target list to tensor\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        return path, target_tensor\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader.\n",
    "    batch: list of (path, target_tensor) tuples\n",
    "    Returns: (list of paths, stacked targets tensor [batch_size, 12])\n",
    "    \"\"\"\n",
    "    paths, targets = zip(*batch)\n",
    "    targets_tensor = torch.stack(targets)\n",
    "    return list(paths), targets_tensor\n",
    "\n",
    "\n",
    "def create_train_test_loaders(\n",
    "    imgs_directory_path, \n",
    "    pretraining_metadata_path, \n",
    "    batch_size_train=32, \n",
    "    batch_size_test=32, \n",
    "    test_percentage=0.1,\n",
    "):\n",
    "    \"\"\"Create dataloaders that return IMAGE PATHS and properly stacked TARGETS.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- Scan folder for images ---\n",
    "    image_paths = []\n",
    "    image_ids = []\n",
    "    all_files = sorted(os.listdir(imgs_directory_path))\n",
    "    \n",
    "    for file_name in all_files:\n",
    "        if file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            path = os.path.join(imgs_directory_path, file_name)\n",
    "            image_paths.append(path)\n",
    "            # Extract ID from filename (first part before underscore)\n",
    "            image_ids.append(file_name.split(\"_\")[0])\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} image files in {imgs_directory_path}\")\n",
    "    \n",
    "    # --- Load metadata ---\n",
    "    with open(pretraining_metadata_path, 'r', encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"Loaded metadata for {len(metadata)} paintings\")\n",
    "    \n",
    "    # --- Match images with valid targets ---\n",
    "    targets = []\n",
    "    valid_paths = []\n",
    "    matched_count = 0\n",
    "    \n",
    "    for path, img_id in zip(image_paths, image_ids):\n",
    "        if img_id in metadata and \"rough_groundtruth\" in metadata[img_id]:\n",
    "            target = metadata[img_id][\"rough_groundtruth\"]\n",
    "            targets.append(target)\n",
    "            valid_paths.append(path)\n",
    "            matched_count += 1\n",
    "    \n",
    "    print(f\"Matched {matched_count}/{len(image_paths)} images with valid targets\")\n",
    "    \n",
    "    if matched_count == 0:\n",
    "        raise ValueError(\"No images matched with metadata! Check your image IDs and metadata format.\")\n",
    "    \n",
    "    # --- Verify target dimensions ---\n",
    "    first_target = targets[0]\n",
    "    target_dim = len(first_target)\n",
    "    print(f\"Target dimension: {target_dim}\")\n",
    "    for i, t in enumerate(targets[:3]):\n",
    "        if len(t) != target_dim:\n",
    "            raise ValueError(f\"Inconsistent target dimensions: image {i} has {len(t)}, expected {target_dim}\")\n",
    "    \n",
    "    # --- Split train/test ---\n",
    "    num_images = len(valid_paths)\n",
    "    num_test = int(num_images * test_percentage)\n",
    "    indices = list(range(num_images))\n",
    "    random.shuffle(indices)\n",
    "    test_indices = set(indices[:num_test])\n",
    "    \n",
    "    train_paths = [valid_paths[i] for i in range(num_images) if i not in test_indices]\n",
    "    train_targets = [targets[i] for i in range(num_images) if i not in test_indices]\n",
    "    test_paths = [valid_paths[i] for i in range(num_images) if i in test_indices]\n",
    "    test_targets = [targets[i] for i in range(num_images) if i in test_indices]\n",
    "    \n",
    "    print(f\"\\nTrain: {len(train_paths)} images\")\n",
    "    print(f\"Test: {len(test_paths)} images\")\n",
    "    print(f\"Sample train path: {train_paths[0]}\")\n",
    "    print(f\"Sample train target: {train_targets[0]}\")\n",
    "    \n",
    "    # --- Create Dataset objects ---\n",
    "    train_dataset = PaintingDataset(train_paths, train_targets)\n",
    "    test_dataset = PaintingDataset(test_paths, test_targets)\n",
    "    \n",
    "    # --- Create DataLoaders with custom collate function ---\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size_test,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # --- Verify DataLoader output ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFYING DATALOADER OUTPUT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for batch_idx, (image_paths_batch, targets_batch) in enumerate(train_loader):\n",
    "        print(f\"\\nBatch {batch_idx}:\")\n",
    "        print(f\"  image_paths: {len(image_paths_batch)} items (type: {type(image_paths_batch)})\")\n",
    "        print(f\"  targets: {targets_batch.shape} (type: {type(targets_batch)})\")\n",
    "        print(f\"  targets dtype: {targets_batch.dtype}\")\n",
    "        print(f\"  targets range: [{targets_batch.min():.2f}, {targets_batch.max():.2f}]\")\n",
    "        print(f\"  First path: {image_paths_batch[0]}\")\n",
    "        print(f\"  First target: {targets_batch[0]}\")\n",
    "        \n",
    "        if batch_idx >= 1:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n✓ DataLoader verification complete!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if PRETRAINING:\n",
    "    train_loader, test_loader = create_train_test_loaders(\n",
    "        imgs_directory_path=imgs_directory_path,\n",
    "        pretraining_metadata_path=pretraining_metadata,\n",
    "        batch_size_train=32,\n",
    "        batch_size_test=32,\n",
    "        test_percentage=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6be0459",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_gpu_mem(prefix=\"GPU\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2   # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2     # MB\n",
    "        print(f\"{prefix} Memory — Allocated: {allocated:.2f} MB | Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766b12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BLIP2MultiHeadRegression(nn.Module):\n",
    "    def __init__(self, blip2_model,\n",
    "                 use_style_head=True,\n",
    "                 train_qformer=False,\n",
    "                 train_vision=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Core model ---\n",
    "        self.blip2 = blip2_model\n",
    "        self.use_style_head = use_style_head\n",
    "\n",
    "        # --- Control what's trainable ---\n",
    "        for param in self.blip2.vision_model.parameters():\n",
    "            param.requires_grad = train_vision\n",
    "        for param in self.blip2.qformer.parameters():\n",
    "            param.requires_grad = train_qformer\n",
    "\n",
    "        # --- Move modules to appropriate devices ---\n",
    "        self.blip2.vision_model.to(VISION_DEVICE)\n",
    "        self.blip2.qformer.to(MAIN_DEVICE)\n",
    "\n",
    "        # query_tokens is an nn.Parameter → rewrap properly after moving\n",
    "        self.blip2.query_tokens = nn.Parameter(\n",
    "            self.blip2.query_tokens.to(MAIN_DEVICE)\n",
    "        )\n",
    "\n",
    "        # --- Config info ---\n",
    "        num_query_tokens = blip2_model.config.num_query_tokens\n",
    "        hidden_size = blip2_model.config.qformer_config.hidden_size\n",
    "        feature_dim = num_query_tokens * hidden_size\n",
    "\n",
    "        print(f\"Num query tokens: {num_query_tokens}\")\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        print(f\"Feature dim: {feature_dim}\")\n",
    "        print(f\"Use style head: {use_style_head}\")\n",
    "\n",
    "        # --- Shared feature extraction ---\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "        # --- Movement and Genre heads ---\n",
    "        self.movement_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, MOVEMENT_DIM)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "        self.genre_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, GENRE_DIM)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "        # --- Style head (always defined, but only used if enabled) ---\n",
    "        self.style_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, STYLE_DIM)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "    def forward(self, images, return_features=False):\n",
    "        \"\"\"\n",
    "        Forward pass with optional CPU/GPU split for vision model.\n",
    "\n",
    "        Args:\n",
    "            images: [batch_size, 3, H, W]\n",
    "            return_features: If True, also return shared features\n",
    "\n",
    "        Returns:\n",
    "            dict with keys: 'movement', 'genre', 'style', 'combined', optionally 'features'\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Vision encoding ---\n",
    "        images_vision = images.to(VISION_DEVICE)\n",
    "\n",
    "        if self.training and next(self.blip2.vision_model.parameters()).requires_grad:\n",
    "            vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "\n",
    "        image_embeds = vision_outputs.last_hidden_state.to(MAIN_DEVICE)  # move to GPU\n",
    "\n",
    "        # --- Q-Former processing ---\n",
    "        query_tokens = self.blip2.query_tokens.expand(images.shape[0], -1, -1).to(MAIN_DEVICE)\n",
    "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long).to(MAIN_DEVICE)\n",
    "\n",
    "        query_outputs = self.blip2.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # --- Flatten Q-Former output ---\n",
    "        query_hidden_states = query_outputs.last_hidden_state\n",
    "        flattened = query_hidden_states.flatten(start_dim=1)\n",
    "\n",
    "        # --- Shared features ---\n",
    "        shared_features = self.shared_features(flattened)\n",
    "\n",
    "        # --- Regression heads ---\n",
    "        movement_scores = self.movement_head(shared_features)\n",
    "        genre_scores = self.genre_head(shared_features)\n",
    "        style_scores = self.style_head(shared_features)\n",
    "\n",
    "        outputs = {\n",
    "            'movement': movement_scores,\n",
    "            'genre': genre_scores,\n",
    "            'style': style_scores,\n",
    "            'combined': torch.cat([movement_scores, genre_scores, style_scores], dim=1)\n",
    "        }\n",
    "\n",
    "        if return_features:\n",
    "            outputs['features'] = shared_features\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class WeightedMultiHeadLoss(nn.Module):\n",
    "    def __init__(self, movement_weight=1.0, genre_weight=1.0, style_weight=1.0, use_style=True):\n",
    "        super().__init__()\n",
    "        self.movement_weight = movement_weight\n",
    "        self.genre_weight = genre_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.use_style = use_style\n",
    "\n",
    "        # BCE loss for binary targets\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, predictions, targets, confidences=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: dict with 'movement', 'genre', 'style' (raw logits)\n",
    "            targets: tensor [batch, total_dim] (binary targets 0 or 1)\n",
    "            confidences: dict with confidence scores (optional)\n",
    "        \"\"\"\n",
    "        # Split targets by head dimensions\n",
    "        movement_target = targets[:, :MOVEMENT_DIM]\n",
    "        genre_target    = targets[:, MOVEMENT_DIM : MOVEMENT_DIM + GENRE_DIM]\n",
    "\n",
    "        # --- Movement loss ---\n",
    "        movement_loss = self.bce(predictions['movement'], movement_target)\n",
    "        if confidences is not None and 'movement' in confidences:\n",
    "            movement_loss = movement_loss * confidences['movement']\n",
    "        movement_loss = movement_loss.mean() * self.movement_weight\n",
    "\n",
    "        # --- Genre loss ---\n",
    "        genre_loss = self.bce(predictions['genre'], genre_target)\n",
    "        if confidences is not None and 'genre' in confidences:\n",
    "            genre_loss = genre_loss * confidences['genre']\n",
    "        genre_loss = genre_loss.mean() * self.genre_weight\n",
    "\n",
    "        # --- Total loss ---\n",
    "        total_loss = movement_loss + genre_loss\n",
    "        loss_dict = {'movement': movement_loss.item(), 'genre': genre_loss.item()}\n",
    "\n",
    "        # --- Style loss (optional) ---\n",
    "        if self.use_style:\n",
    "            style_target = targets[:, MOVEMENT_DIM + GENRE_DIM :]\n",
    "            style_loss = self.bce(predictions['style'], style_target)\n",
    "            if confidences is not None and 'style' in confidences:\n",
    "                style_loss = style_loss * confidences['style']\n",
    "            style_loss = style_loss.mean() * self.style_weight\n",
    "            total_loss += style_loss\n",
    "            loss_dict['style'] = style_loss.item()\n",
    "\n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        return total_loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps, UnidentifiedImageError, ImageFile\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Do NOT allow truncated images - raise errors instead\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "\n",
    "def augment_batch(image_paths, targets, processor):\n",
    "    \"\"\"\n",
    "    Load images, create flipped versions, and return pixel values.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of file paths to images (length batch_size)\n",
    "        targets: Tensor of shape [batch_size, 12]\n",
    "        processor: BLIP2 processor\n",
    "        \n",
    "    Returns:\n",
    "        pixel_values: Tensor of shape [batch_size*2, 3, H, W]\n",
    "        doubled_targets: Tensor of shape [batch_size*2, 12]\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    doubled_targets = []\n",
    "    \n",
    "    # Iterate by index since targets is a tensor\n",
    "    for idx in range(len(image_paths)):\n",
    "        img_path = image_paths[idx]\n",
    "        target = targets[idx]  # Get row from tensor\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except (OSError, UnidentifiedImageError) as e:\n",
    "            print(f\"⚠️ Skipping corrupted image: {img_path} ({e})\")\n",
    "            continue\n",
    "        \n",
    "        # Add original image\n",
    "        images.append(img)\n",
    "        doubled_targets.append(target)\n",
    "        \n",
    "        # Add horizontally flipped image\n",
    "        flipped_img = ImageOps.mirror(img)\n",
    "        images.append(flipped_img)\n",
    "        doubled_targets.append(target)\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Process all images at once with processor\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    # Stack all target rows into [num_images, 12]\n",
    "    targets_tensor = torch.stack(doubled_targets)\n",
    "    \n",
    "    return pixel_values, targets_tensorfrom PIL import Image, ImageOps, UnidentifiedImageError, ImageFile\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Do NOT allow truncated images - raise errors instead\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "\n",
    "def augment_batch(image_paths, targets, processor):\n",
    "    \"\"\"\n",
    "    Load images, create flipped versions, and return pixel values.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of file paths to images (length batch_size)\n",
    "        targets: Tensor of shape [batch_size, 12]\n",
    "        processor: BLIP2 processor\n",
    "        \n",
    "    Returns:\n",
    "        pixel_values: Tensor of shape [batch_size*2, 3, H, W]\n",
    "        doubled_targets: Tensor of shape [batch_size*2, 12]\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    doubled_targets = []\n",
    "    \n",
    "    # Iterate by index since targets is a tensor\n",
    "    for idx in range(len(image_paths)):\n",
    "        img_path = image_paths[idx]\n",
    "        target = targets[idx]  # Get row from tensor\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except (OSError, UnidentifiedImageError) as e:\n",
    "            print(f\"⚠️ Skipping corrupted image: {img_path} ({e})\")\n",
    "            continue\n",
    "        \n",
    "        # Add original image\n",
    "        images.append(img)\n",
    "        doubled_targets.append(target)\n",
    "        \n",
    "        # Add horizontally flipped image\n",
    "        flipped_img = ImageOps.mirror(img)\n",
    "        images.append(flipped_img)\n",
    "        doubled_targets.append(target)\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Process all images at once with processor\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    # Stack all target rows into [num_images, 12]\n",
    "    targets_tensor = torch.stack(doubled_targets)\n",
    "    \n",
    "    return pixel_values, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e033942c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, processor):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    images_processed = 0\n",
    "    for step, (image_paths, targets) in enumerate(dataloader):\n",
    "        print(f\"step: {step}, image_paths: {image_paths}, targets: {targets}\")\n",
    "        # Augment the batch\n",
    "        pixel_values, targets_tensor = augment_batch(image_paths, targets, processor)\n",
    "        if pixel_values == None:\n",
    "            continue\n",
    "        \n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        targets_tensor = targets_tensor.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values)\n",
    "        loss, loss_dict = criterion(predictions, targets_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        time_elapsed = time.time() - start_time\n",
    "\n",
    "        images_processed += len(pixel_values)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "        # if True:\n",
    "            print(f\"Step {step + 1}/{len(dataloader)} | Images: {images_processed} | Time: {time_elapsed:.2f}s | Loss: {loss.item():.4f}\")\n",
    "            print_gpu_mem()\n",
    "        \n",
    "    num_batches = len(dataloader)\n",
    "    total_images = num_batches * dataloader.batch_size * 2  # *2 for augmentation\n",
    "    avg_loss = total_loss / num_batches\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch complete | Avg Loss: {avg_loss:.4f} | Total images: {total_images} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3e6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, dataloader, criterion, device, processor):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image_paths, targets in dataloader:\n",
    "            # Augment the batch (same as training for consistency)\n",
    "            pixel_values, targets_tensor = augment_batch(image_paths, targets, processor)\n",
    "            \n",
    "            pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "            targets_tensor = targets_tensor.to(device, non_blocking=True)\n",
    "            \n",
    "            predictions = model(pixel_values)\n",
    "            loss, _ = criterion(predictions, targets_tensor)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation complete | Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2100ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "def save_progress(model, save_path):\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # e.g., 20251013_170512\n",
    "    checkpoint_file = os.path.join(save_path, f\"model_{time_str}.pt\")\n",
    "\n",
    "    state_dict = {\n",
    "        \"shared_features\": model.shared_features.state_dict(),\n",
    "        \"movement_head\": model.movement_head.state_dict(),\n",
    "        \"genre_head\": model.genre_head.state_dict(),\n",
    "        \"style_head\": model.style_head.state_dict(),\n",
    "    }\n",
    "\n",
    "    # Optionally include Q-Former if it's being trained\n",
    "    if any(p.requires_grad for p in model.blip2.qformer.parameters()):\n",
    "        state_dict[\"qformer\"] = model.blip2.qformer.state_dict()\n",
    "        \n",
    "    torch.save(state_dict, checkpoint_file)\n",
    "    print(f\"✅ Saved fine-tuned modules to: {checkpoint_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc7d3d5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, processor, \n",
    "                num_epochs=10, save_path=None, scheduler=None, early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    Train the model with optional learning rate scheduling and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "        device: Device to train on\n",
    "        processor: BLIP2 processor for augmentation\n",
    "        num_epochs: Number of epochs to train\n",
    "        save_path: Path to save checkpoints\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        early_stopping_patience: Stop if validation loss doesn't improve for N epochs (None = disabled)\n",
    "    \n",
    "    Returns:\n",
    "        history: Dictionary with training history\n",
    "    \"\"\"\n",
    "    print(\"Starting training\")\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"learning_rates\": []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        if scheduler:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Learning rate: {current_lr:.2e}\")\n",
    "            history[\"learning_rates\"].append(current_lr)\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, processor)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            val_loss = test_epoch(model, val_loader, criterion, device, processor)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            \n",
    "            # Check for improvement\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping_patience and epochs_without_improvement >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
    "                print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "        \n",
    "            if save_path is not None:\n",
    "                save_progress(model, save_path)\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training complete\")\n",
    "    if val_loader is not None:\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e121bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATALOADER DEBUG\n",
      "============================================================\n",
      "\n",
      "Batch 0:\n",
      "  image_paths type: <class 'tuple'>, len: 32\n",
      "  image_paths[0]: c:\\proggers\\ArtEmbedding\\paintings\\011851_charles-de-steuben—portrait-of-a-second-lieutenant.jpg\n",
      "  targets type: <class 'list'>, len: 12\n",
      "  targets[0] type: <class 'torch.Tensor'>\n",
      "  targets[0]: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011851_charles-de-steuben—portrait-of-a-second-lieutenant.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\022398_pasquale-celommi—the-crucifixion-1900.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005624_claudio-coello—santa-catalina-de-alejandria-1683.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028521_john-henry-twachtman—winter-landscape-2.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011135_charles-turner—edward-pellew-1st-viscount-exmouth-1815.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024932_alfred-sisley—may-afternoon-on-the-loing-1888.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010108_jean-auguste-dominique-ingres—self-portrait-2.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\029929_lucien-pissarro—le-ragas-near-toulon-1930.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011703_david-wilkie—samuel-in-the-temple.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012305_charlet—la-partie-de-billes-1837.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\030019_joaqu-n-sorolla—pine-trees-1902.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012250_theodore-gericault—rifleman.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025279_claude-monet—the-banks-of-the-seine-lavacourt-02.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014605_james-webb—a-bit-of-sussex-1877.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\001006_peter-paul-rubens—susanna-and-the-elders.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003516_rembrandt—self-portrait-1642.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027279_mary-cassatt—mathilde-holding-a-baby-who-reaches-out-to-the-right-1.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\007351_joshua-reynolds—scyacust-ukah.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010072_jean-auguste-dominique-ingres—the-martyrdom-of-st-symphorian-1834.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027857_gustave-caillebotte—portrait-of-eugene-lamy.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000783_peter-paul-rubens—virgin-and-child-1620.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002269_dirck-van-baburen—saint-james-the-greater.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009274_james-barry—john-wesley.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\016779_john-william-waterhouse—ariadne-1898.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012366_ferdinand-georg-waldm-ller—emanuel-ritter-von-neuwall.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020547_edgar-degas—at-the-races.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008710_vieira-portuense—leda-e-o-cisne-1798.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010796_john-hoppner—lady-louisa-manners-countess-of-dysart-1821.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\029757_frank-w-benson—portrait-of-willard-metcalf-1921.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018438_honore-daumier—the-nocturnal-travellers-1847.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\007587_thomas-gainsborough—st-mary-s-church-hadleigh.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023044_john-singer-sargent—shoeing-the-ox.jpg')\n",
      "[tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64), tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64)]\n",
      "  After augment:\n",
      "    pixel_values shape: torch.Size([24, 3, 224, 224])\n",
      "    targets_tensor shape: torch.Size([24, 32])\n",
      "    targets_tensor dtype: torch.float32\n",
      "    targets_tensor range: [0.0000, 1.0000]\n",
      "    Any NaN? False\n",
      "\n",
      "Batch 1:\n",
      "  image_paths type: <class 'tuple'>, len: 32\n",
      "  image_paths[0]: c:\\proggers\\ArtEmbedding\\paintings\\000555_guido-reni—the-suicide-of-lucretia-1640.jpg\n",
      "  targets type: <class 'list'>, len: 12\n",
      "  targets[0] type: <class 'torch.Tensor'>\n",
      "  targets[0]: tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "       dtype=torch.float64)\n",
      "('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000555_guido-reni—the-suicide-of-lucretia-1640.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014839_oswald-achenbach—via-appia-with-the-tomb-of-caecilia-metell-1886-0.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004764_aelbert-cuyp—the-small-dort.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013152_theodor-leopold-weller—portrait-of-a-lady-1859.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019313_august-von-pettenkofen—gypsy-hut.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008257_charles-willson-peale—thomas-elliott-and-his-granddaughter-deborah-hibernia-1797.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014118_john-ruskin—rochers-de-lanfon-lake-annecy-1863.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\030530_henri-matisse—corsican-landscape-1898-1.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009769_francois-gerard—madame-charles-maurice-de-talleyrand-perigord-1761-1835.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008217_benjamin-west—william-markham.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011093_charles-turner—frances-teresa-stuart-duchess-of-richmond-and-lennox-1810-1.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020104_edouard-manet—the-fifer-1866.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002058_jacob-jordaens—the-bean-king-the-king-drinks-1638.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010603_francisco-goya—count-fernand-nunez-vii-1803.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000108_annibale-carracci—landscape-with-the-sacrifice-of-isaac.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009792_francois-gerard—thetis-delivering-achilles-new-armor.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027934_william-merritt-chase—portrait-of-william-gurley-munson-1868.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020898_henri-fantin-latour—homage-to-delacroix-1864.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025643_claude-monet—juan-les-pins.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\007843_jean-honore-fragonard—the-beginnings-of-model.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\030089_joaqu-n-sorolla—marian-in-the-gardens-la-granja-1907.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\022323_eugene-burnand—heimgefunden-1900.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012666_david-roberts—the-rock-of-moses.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\016030_thomas-moran—index-peak.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000942_peter-paul-rubens—the-triptych-of-saint-ildefonso-altar.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003018_matthias-stom—adoration-of-the-magi.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\029222_childe-hassam—in-the-garden-1889.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018699_charles-jacque—musician-1847.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012335_ferdinand-georg-waldm-ller—dachstein-from-the-place-of-sophie-1835.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\001291_frans-hals—portrait-of-a-forty-year-old-woman-with-folded-hands-1638.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025556_claude-monet—the-road-to-giverny-2.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003636_rembrandt—anatomy-of-doctor-deyman-1656.jpg')\n",
      "[tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       dtype=torch.float64)]\n",
      "  After augment:\n",
      "    pixel_values shape: torch.Size([24, 3, 224, 224])\n",
      "    targets_tensor shape: torch.Size([24, 32])\n",
      "    targets_tensor dtype: torch.float32\n",
      "    targets_tensor range: [0.0000, 1.0000]\n",
      "    Any NaN? False\n",
      "\n",
      "Batch 2:\n",
      "  image_paths type: <class 'tuple'>, len: 32\n",
      "  image_paths[0]: c:\\proggers\\ArtEmbedding\\paintings\\009798_francois-gerard—ritratto-della-cantante-giuditta-pasta.jpg\n",
      "  targets type: <class 'list'>, len: 12\n",
      "  targets[0] type: <class 'torch.Tensor'>\n",
      "  targets[0]: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009798_francois-gerard—ritratto-della-cantante-giuditta-pasta.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\007353_joshua-reynolds—mrs-thomas-riddell.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\001479_cornelis-de-vos—portrait-of-a-woman.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018779_jean-francois-millet—garden-scene.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027320_mary-cassatt—peasant-mother-and-child-1894.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014204_ford-madox-brown—first-observation-of-the-transit-of-venus-by-william-crabtree-in-1639.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018651_theodore-rousseau—morning.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018383_george-catlin—a-village-of-the-hidatsa-tribe-at-knife-river-1832.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014563_adolphe-joseph-thomas-monticelli—oriental-scene-1876.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024415_camille-pissarro—grey-weather-morning-with-figures-eragny-1899.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008722_hendrik-voogd—italian-landscape-1819.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\021651_giovanni-boldini—self-portrait-1892.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014947_jean-baptiste-carpeaux—attentat-de-berezowski-contre-le-tsar-alexandre-ii.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010515_francisco-goya—the-school-scene.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\017366_charles-m-russell—capturing-the-grizzly-1901.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023858_eugene-boudin—berck-fisherwomen-on-the-beach.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005751_francesco-solimena—aurora-roman-goddess-of-the-dawn-bids-goodbye-to-her-lover-tithonus-aurora-is-about-to.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\026485_pierre-auguste-renoir—marie-therese-durand-ruel-sewing-1882.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025987_berthe-morisot—portrait-of-a-young-lady.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000573_guido-reni—adoration-of-the-shepherds.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003719_rembrandt—portrait-of-a-man-1667.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019340_alfred-stevens—the-paris-sfinks-1867.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023102_john-singer-sargent—palmettos-florida-1917.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019504_eastman-johnson—sarah-osgood-johnson-newton-1856.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019186_gustave-courbet—view-of-the-forest-of-fontainebleau.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000048_agostino-carracci—hairy-harry-mad-peter-and-tiny-amon-1600.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027284_mary-cassatt—maternity.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027950_william-merritt-chase—the-mandolin-player.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\021009_winslow-homer—north-road-bermuda.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012374_ferdinand-georg-waldm-ller—rosina-wieser-in-armchair.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\017487_charles-m-russell—piegans-1918.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013028_heinrich-burkel—farm-in-front-of-garmisch-in-the-background-the-wetterstein-mountains.jpg')\n",
      "[tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "       dtype=torch.float64)]\n",
      "  After augment:\n",
      "    pixel_values shape: torch.Size([24, 3, 224, 224])\n",
      "    targets_tensor shape: torch.Size([24, 32])\n",
      "    targets_tensor dtype: torch.float32\n",
      "    targets_tensor range: [0.0000, 1.0000]\n",
      "    Any NaN? False\n",
      "\n",
      "============================================================\n",
      "METADATA DEBUG\n",
      "============================================================\n",
      "\n",
      "Image: 000001\n",
      "  Target type: <class 'list'>\n",
      "  Target: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "  Length: 12\n",
      "  Values: min=0.0, max=1.0\n",
      "\n",
      "Image: 000002\n",
      "  Target type: <class 'list'>\n",
      "  Target: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Length: 12\n",
      "  Values: min=0.0, max=1.0\n",
      "\n",
      "Image: 000003\n",
      "  Target type: <class 'list'>\n",
      "  Target: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "  Length: 12\n",
      "  Values: min=0.0, max=1.0\n",
      "\n",
      "Image: 000004\n",
      "  Target type: <class 'list'>\n",
      "  Target: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "  Length: 12\n",
      "  Values: min=0.0, max=1.0\n",
      "\n",
      "Image: 000005\n",
      "  Target type: <class 'list'>\n",
      "  Target: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "  Length: 12\n",
      "  Values: min=0.0, max=1.0\n",
      "\n",
      "============================================================\n",
      "LOSS FUNCTION DEBUG\n",
      "============================================================\n",
      "Predictions shapes:\n",
      "  movement: torch.Size([4, 6])\n",
      "  genre: torch.Size([4, 6])\n",
      "  style: torch.Size([4, 6])\n",
      "Targets shape: torch.Size([4, 12])\n",
      "Targets range: [0.0, 1.0]\n",
      "\n",
      "Loss computation successful:\n",
      "  Total loss: 1.7712\n",
      "  Loss dict: {'movement': 0.836921215057373, 'genre': 0.9342871904373169, 'total': 1.77120840549469}\n",
      "\n",
      "Testing with continuous targets (0-1 range, not binary):\n",
      "  Loss: 1.7370\n",
      "  Loss dict: {'movement': 0.866492509841919, 'genre': 0.87052983045578, 'total': 1.7370223999023438}\n",
      "\n",
      "============================================================\n",
      "DEBUG COMPLETE - Check outputs above for issues\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Add these debugging functions to diagnose the issues\n",
    "\n",
    "def debug_dataloader(train_loader, num_batches=3):\n",
    "    \"\"\"Inspect what the dataloader is actually returning\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DATALOADER DEBUG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for batch_idx, (image_paths, targets) in enumerate(train_loader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nBatch {batch_idx}:\")\n",
    "        print(f\"  image_paths type: {type(image_paths)}, len: {len(image_paths)}\")\n",
    "        print(f\"  image_paths[0]: {image_paths[0]}\")\n",
    "        print(f\"  targets type: {type(targets)}, len: {len(targets)}\")\n",
    "        print(f\"  targets[0] type: {type(targets[0])}\")\n",
    "        print(f\"  targets[0]: {targets[0]}\")\n",
    "        \n",
    "        # Try augmentation\n",
    "        pixel_values, targets_tensor = augment_batch(image_paths, targets, processor)\n",
    "        if pixel_values is not None:\n",
    "            print(f\"  After augment:\")\n",
    "            print(f\"    pixel_values shape: {pixel_values.shape}\")\n",
    "            print(f\"    targets_tensor shape: {targets_tensor.shape}\")\n",
    "            print(f\"    targets_tensor dtype: {targets_tensor.dtype}\")\n",
    "            print(f\"    targets_tensor range: [{targets_tensor.min():.4f}, {targets_tensor.max():.4f}]\")\n",
    "            print(f\"    Any NaN? {torch.isnan(targets_tensor).any()}\")\n",
    "        else:\n",
    "            print(f\"  Augmentation returned None (all images corrupted?)\")\n",
    "\n",
    "# Run this first\n",
    "debug_dataloader(train_loader, num_batches=3)\n",
    "\n",
    "# Then check your targets in metadata\n",
    "def debug_metadata(pretraining_metadata, num_samples=5):\n",
    "    \"\"\"Check what targets look like in metadata\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"METADATA DEBUG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    count = 0\n",
    "    for img_id, metadata in pretraining_metadata.items():\n",
    "        if count >= num_samples:\n",
    "            break\n",
    "        \n",
    "        if \"rough_groundtruth\" in metadata:\n",
    "            target = metadata[\"rough_groundtruth\"]\n",
    "            print(f\"\\nImage: {img_id}\")\n",
    "            print(f\"  Target type: {type(target)}\")\n",
    "            print(f\"  Target: {target}\")\n",
    "            print(f\"  Length: {len(target) if isinstance(target, (list, tuple)) else 'N/A'}\")\n",
    "            print(f\"  Values: min={min(target) if isinstance(target, (list, tuple)) else 'N/A'}, \"\n",
    "                  f\"max={max(target) if isinstance(target, (list, tuple)) else 'N/A'}\")\n",
    "            count += 1\n",
    "\n",
    "with open(pretraining_metadata, 'r', encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "debug_metadata(metadata, num_samples=5)\n",
    "\n",
    "# Check loss computation\n",
    "def debug_loss_computation():\n",
    "    \"\"\"Test loss function with dummy data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOSS FUNCTION DEBUG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    criterion = WeightedMultiHeadLoss(\n",
    "        movement_weight=1.0,\n",
    "        genre_weight=1.0,\n",
    "        use_style=False\n",
    "    ).to(MAIN_DEVICE)\n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    # Create dummy predictions\n",
    "    dummy_preds = {\n",
    "        'movement': torch.randn(batch_size, MOVEMENT_DIM).to(MAIN_DEVICE),\n",
    "        'genre': torch.randn(batch_size, GENRE_DIM).to(MAIN_DEVICE),\n",
    "        'style': torch.randn(batch_size, STYLE_DIM).to(MAIN_DEVICE),\n",
    "    }\n",
    "    \n",
    "    # Create dummy targets (binary 0 or 1)\n",
    "    dummy_targets = torch.randint(0, 2, (batch_size, MOVEMENT_DIM + GENRE_DIM)).float().to(MAIN_DEVICE)\n",
    "    \n",
    "    print(f\"Predictions shapes:\")\n",
    "    for k, v in dummy_preds.items():\n",
    "        print(f\"  {k}: {v.shape}\")\n",
    "    \n",
    "    print(f\"Targets shape: {dummy_targets.shape}\")\n",
    "    print(f\"Targets range: [{dummy_targets.min()}, {dummy_targets.max()}]\")\n",
    "    \n",
    "    loss, loss_dict = criterion(dummy_preds, dummy_targets)\n",
    "    print(f\"\\nLoss computation successful:\")\n",
    "    print(f\"  Total loss: {loss.item():.4f}\")\n",
    "    print(f\"  Loss dict: {loss_dict}\")\n",
    "    \n",
    "    # Now test with bad targets\n",
    "    print(f\"\\nTesting with continuous targets (0-1 range, not binary):\")\n",
    "    bad_targets = torch.rand(batch_size, MOVEMENT_DIM + GENRE_DIM).to(MAIN_DEVICE)\n",
    "    loss2, loss_dict2 = criterion(dummy_preds, bad_targets)\n",
    "    print(f\"  Loss: {loss2.item():.4f}\")\n",
    "    print(f\"  Loss dict: {loss_dict2}\")\n",
    "\n",
    "debug_loss_computation()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEBUG COMPLETE - Check outputs above for issues\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5785530",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRETRAINING MODE (no style head)\n",
      "============================================================\n",
      "Num query tokens: 32\n",
      "Hidden size: 768\n",
      "Feature dim: 24576\n",
      "Use style head: False\n",
      "No checkpoint found. Starting from scratch.\n",
      "Starting training\n",
      "\n",
      "============================================================\n",
      "Epoch 1/5\n",
      "Learning rate: 1.00e-04\n",
      "============================================================\n",
      "step: 0, image_paths: ('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008953_claude-joseph-vernet—seascape-view-of-sorrento.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003657_rembrandt—christ-and-the-samaritan-at-the-well-1659.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002501_francisco-de-zurbaran—brother-alonso-de-ocana.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024747_alfred-sisley—the-seine-at-marly-1876.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\022795_john-singer-sargent—helen-sears-1895.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024491_camille-pissarro—peasant-woman-at-the-well.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013897_andreas-achenbach—rolands-arch-1834.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028950_willard-metcalf—the-white-veil-1909.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027876_gustave-caillebotte—petit-gennevilliers-facade-southeast-of-the-artist-s-studio-overlooking-the-garden-spring.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013599_paul-kane—indian-camp-colville.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002312_jan-van-goyen—a-river-landscape-with-a-fully-laden-ferry-boat-approaching.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003813_heinrich-schonfeld—allegory-of-time-1630.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025207_claude-monet—in-the-garden.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\016276_john-pettie—a-lady-of-the-seventeenth-century-1877.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\017402_charles-m-russell—the-bucker-1904.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019088_gustave-courbet—going-fishing-1865.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004740_charles-le-brun—hercules-slaying-the-centaurs.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\006766_jean-etienne-liotard—portrait-of-a-lady.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027828_gustave-caillebotte—landscape-study-in-yellow-and-rose.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002525_le-nain-brothers—bacchus-and-ariadne.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011035_louis-philippe-crepin—french-corvette-bayonnaise-boarding-hms-ambuscade-during-the-action-of-14-december-1798-0.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028860_henry-scott-tuke—man-looking-through-a-telescope.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023293_william-logsdail—goldwynne-1935.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011189_charles-turner—frederick-john-robinson-1st-earl-of-ripon-1824.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015931_thomas-moran—grand-canyon-of-the-yellowstone-1904.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018669_theodore-rousseau—landscape-5.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008199_benjamin-west—joshua-passing-the-river-jordan-with-the-ark-of-the-covenant.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005448_luca-giordano—prudent-abigail.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000540_guido-reni—repentant-peter-1637.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023035_john-singer-sargent—girls-gathering-blossoms-valdemosa-majorca-1910.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024422_camille-pissarro—the-field-by-the-ango-inn-varengeville-1899.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012401_ferdinand-georg-waldm-ller—portrait-de-madame-josef-bayer.jpg'), targets: [tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64)]\n",
      "('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008953_claude-joseph-vernet—seascape-view-of-sorrento.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003657_rembrandt—christ-and-the-samaritan-at-the-well-1659.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002501_francisco-de-zurbaran—brother-alonso-de-ocana.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024747_alfred-sisley—the-seine-at-marly-1876.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\022795_john-singer-sargent—helen-sears-1895.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024491_camille-pissarro—peasant-woman-at-the-well.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013897_andreas-achenbach—rolands-arch-1834.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028950_willard-metcalf—the-white-veil-1909.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027876_gustave-caillebotte—petit-gennevilliers-facade-southeast-of-the-artist-s-studio-overlooking-the-garden-spring.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013599_paul-kane—indian-camp-colville.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002312_jan-van-goyen—a-river-landscape-with-a-fully-laden-ferry-boat-approaching.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003813_heinrich-schonfeld—allegory-of-time-1630.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025207_claude-monet—in-the-garden.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\016276_john-pettie—a-lady-of-the-seventeenth-century-1877.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\017402_charles-m-russell—the-bucker-1904.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019088_gustave-courbet—going-fishing-1865.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004740_charles-le-brun—hercules-slaying-the-centaurs.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\006766_jean-etienne-liotard—portrait-of-a-lady.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027828_gustave-caillebotte—landscape-study-in-yellow-and-rose.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002525_le-nain-brothers—bacchus-and-ariadne.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011035_louis-philippe-crepin—french-corvette-bayonnaise-boarding-hms-ambuscade-during-the-action-of-14-december-1798-0.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028860_henry-scott-tuke—man-looking-through-a-telescope.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023293_william-logsdail—goldwynne-1935.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\011189_charles-turner—frederick-john-robinson-1st-earl-of-ripon-1824.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015931_thomas-moran—grand-canyon-of-the-yellowstone-1904.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018669_theodore-rousseau—landscape-5.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008199_benjamin-west—joshua-passing-the-river-jordan-with-the-ark-of-the-covenant.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005448_luca-giordano—prudent-abigail.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000540_guido-reni—repentant-peter-1637.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023035_john-singer-sargent—girls-gathering-blossoms-valdemosa-majorca-1910.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\024422_camille-pissarro—the-field-by-the-ango-inn-varengeville-1899.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012401_ferdinand-georg-waldm-ller—portrait-de-madame-josef-bayer.jpg')\n",
      "[tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64)]\n",
      "Step 1/844 | Images: 24 | Time: 89.17s | Loss: 1.4239\n",
      "GPU Memory — Allocated: 16570.74 MB | Reserved: 17206.00 MB\n",
      "step: 1, image_paths: ('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005953_canaletto—old-walton-bridge-over-the-thames-1754.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002450_francisco-de-zurbaran—saint-bruno-and-pope-urban-ii.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015179_john-everett-millais—chill-october.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\026768_pierre-auguste-renoir—bust-of-a-girl-1900.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015454_gustave-dore—st-paulrescuedfromthemultitude.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003730_rembrandt—the-flight-into-egypt-0.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005664_michiel-van-musscher—portrait-of-aleksandr-danilovich-menshikov-1698.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000628_peter-paul-rubens—saint-gregory-with-saints-domitilla-maurus-and-papianus-1607.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\001925_gerard-van-honthorst—prinzessin-maria-von-oranien-nassau-1648.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005159_adam-van-der-meulen—lodewijk-xiv-trekt-bij-lobith-nederland-binnen-1690.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020850_telemaco-signorini—pastures-in-castiglioncello-1861.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005101_pieter-de-hooch—a-woman-and-a-maid-in-a-courtyard.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023090_john-singer-sargent—the-fence-1914.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\026356_pierre-auguste-renoir—paul-meunier-1877.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\006532_william-hogarth—strolling-actresses-dressing-in-barn-1738.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010069_jean-auguste-dominique-ingres—study-for-the-martyrdom-of-st-symphorien-1834.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028524_john-henry-twachtman—connecticut-landscape-1895.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009006_joseph-wright—dressing-the-kitten.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015882_laslett-john-pott—the-beloveds-arrival.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020414_edgar-degas—the-savoy-girl-1873.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009200_john-singleton-copley—mr-joshua-henshaw-ii.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\030049_joaqu-n-sorolla—isla-del-cap-marti-javea-1905.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008582_lemuel-francis-abbott—portrait-of-admiral-sir-robert-bruce-kingsmill-bt.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002930_diego-velazquez—the-rokeby-venus-1648.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018865_charles-francois-daubigny—the-crossroads-at-the-eagle-nest-forest-of-fontainebleau.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018930_charles-francois-daubigny—le-ru-de-valmondois.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023569_julio-romero-de-torres—nun-1911.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000333_caravaggio—saint-jerome-writing-1607.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\006412_francois-lemoyne—la-continence-de-scipion.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003242_rembrandt—old-man-with-turban.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008246_charles-willson-peale—william-smith-and-his-grandson-1788.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002177_jacob-jordaens—self-portrait.jpg'), targets: [tensor([1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.],\n",
      "       dtype=torch.float64)]\n",
      "('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005953_canaletto—old-walton-bridge-over-the-thames-1754.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002450_francisco-de-zurbaran—saint-bruno-and-pope-urban-ii.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015179_john-everett-millais—chill-october.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\026768_pierre-auguste-renoir—bust-of-a-girl-1900.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015454_gustave-dore—st-paulrescuedfromthemultitude.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003730_rembrandt—the-flight-into-egypt-0.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005664_michiel-van-musscher—portrait-of-aleksandr-danilovich-menshikov-1698.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000628_peter-paul-rubens—saint-gregory-with-saints-domitilla-maurus-and-papianus-1607.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\001925_gerard-van-honthorst—prinzessin-maria-von-oranien-nassau-1648.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005159_adam-van-der-meulen—lodewijk-xiv-trekt-bij-lobith-nederland-binnen-1690.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020850_telemaco-signorini—pastures-in-castiglioncello-1861.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005101_pieter-de-hooch—a-woman-and-a-maid-in-a-courtyard.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023090_john-singer-sargent—the-fence-1914.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\026356_pierre-auguste-renoir—paul-meunier-1877.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\006532_william-hogarth—strolling-actresses-dressing-in-barn-1738.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010069_jean-auguste-dominique-ingres—study-for-the-martyrdom-of-st-symphorien-1834.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028524_john-henry-twachtman—connecticut-landscape-1895.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009006_joseph-wright—dressing-the-kitten.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\015882_laslett-john-pott—the-beloveds-arrival.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020414_edgar-degas—the-savoy-girl-1873.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009200_john-singleton-copley—mr-joshua-henshaw-ii.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\030049_joaqu-n-sorolla—isla-del-cap-marti-javea-1905.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008582_lemuel-francis-abbott—portrait-of-admiral-sir-robert-bruce-kingsmill-bt.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002930_diego-velazquez—the-rokeby-venus-1648.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018865_charles-francois-daubigny—the-crossroads-at-the-eagle-nest-forest-of-fontainebleau.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\018930_charles-francois-daubigny—le-ru-de-valmondois.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023569_julio-romero-de-torres—nun-1911.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\000333_caravaggio—saint-jerome-writing-1607.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\006412_francois-lemoyne—la-continence-de-scipion.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003242_rembrandt—old-man-with-turban.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\008246_charles-willson-peale—william-smith-and-his-grandson-1788.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002177_jacob-jordaens—self-portrait.jpg')\n",
      "[tensor([1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.],\n",
      "       dtype=torch.float64)]\n",
      "step: 2, image_paths: ('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009687_domingos-sequeira—the-worship-of-the-mages-1828.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\017358_charles-m-russell—peaceful-valley-saloon-1900.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\021919_thomas-eakins—mrs-richard-day.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005100_pieter-de-hooch—nursing-mother.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013928_andreas-achenbach—landscape-in-hessen-1868.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003200_adriaen-brouwer—smallholders-playing-dice.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002341_juan-van-der-hamen—francisco-de-la-cueva-1625.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025562_claude-monet—the-seine-near-giverny.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\001731_jusepe-de-ribera—saint-elias-1638.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027967_william-merritt-chase—the-consultation.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010056_jean-auguste-dominique-ingres—the-apotheosis-of-homer-1827.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020549_edgar-degas—beach-scene-1877.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\029075_lovis-corinth—the-walchensee-new-snow-1922.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\007466_joshua-reynolds—lady-elizabeth-foster.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004661_bartolome-esteban-murillo—the-madonna-of-the-rosary.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009105_joseph-wright—the-eruption-of-vesuvius.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020282_anton-romako—babette-tapecierer-1876.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019254_august-von-pettenkofen—gypsy-woman-with-two-children-1854.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009966_jerome-martin-langlois—generosite-dalexandre-1819.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004810_juan-de-valdes-leal—miracle-of-st-ildefonsus-1661.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028318_theodore-robinson—old-church-at-giverny-1891.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023538_isaac-israels—pieter-klazes-pel.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\030700_firmin-baes—motherhood.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013973_adolphe-yvon—prise-de-la-tour-de-malakoff-par-le-general-mac-mahon-le-8-septembre-1855-par-adolphe-yvon-1857.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009051_joseph-wright—brooke-boothby.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014413_william-hart—autumn-new-hampshire.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004174_gerrit-dou—the-night-school-1665.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027054_pierre-auguste-renoir—landscape-2.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003726_rembrandt—self-portrait-in-at-the-age-of-63-1669.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003471_rembrandt—jacob-telling-his-dreams-1638.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\029014_lovis-corinth—in-the-woods-near-bernried-1892.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012716_eugene-delacroix—the-natchez-1825.jpg'), targets: [tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64)]\n",
      "('c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009687_domingos-sequeira—the-worship-of-the-mages-1828.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\017358_charles-m-russell—peaceful-valley-saloon-1900.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\021919_thomas-eakins—mrs-richard-day.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\005100_pieter-de-hooch—nursing-mother.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013928_andreas-achenbach—landscape-in-hessen-1868.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003200_adriaen-brouwer—smallholders-playing-dice.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\002341_juan-van-der-hamen—francisco-de-la-cueva-1625.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\025562_claude-monet—the-seine-near-giverny.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\001731_jusepe-de-ribera—saint-elias-1638.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027967_william-merritt-chase—the-consultation.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\010056_jean-auguste-dominique-ingres—the-apotheosis-of-homer-1827.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020549_edgar-degas—beach-scene-1877.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\029075_lovis-corinth—the-walchensee-new-snow-1922.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\007466_joshua-reynolds—lady-elizabeth-foster.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004661_bartolome-esteban-murillo—the-madonna-of-the-rosary.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009105_joseph-wright—the-eruption-of-vesuvius.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\020282_anton-romako—babette-tapecierer-1876.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\019254_august-von-pettenkofen—gypsy-woman-with-two-children-1854.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009966_jerome-martin-langlois—generosite-dalexandre-1819.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004810_juan-de-valdes-leal—miracle-of-st-ildefonsus-1661.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\028318_theodore-robinson—old-church-at-giverny-1891.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\023538_isaac-israels—pieter-klazes-pel.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\030700_firmin-baes—motherhood.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\013973_adolphe-yvon—prise-de-la-tour-de-malakoff-par-le-general-mac-mahon-le-8-septembre-1855-par-adolphe-yvon-1857.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\009051_joseph-wright—brooke-boothby.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\014413_william-hart—autumn-new-hampshire.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\004174_gerrit-dou—the-night-school-1665.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\027054_pierre-auguste-renoir—landscape-2.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003726_rembrandt—self-portrait-in-at-the-age-of-63-1669.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\003471_rembrandt—jacob-telling-his-dreams-1638.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\029014_lovis-corinth—in-the-woods-near-bernried-1892.jpg', 'c:\\\\proggers\\\\ArtEmbedding\\\\paintings\\\\012716_eugene-delacroix—the-natchez-1825.jpg')\n",
      "[tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64), tensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
      "       dtype=torch.float64), tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
      "       dtype=torch.float64), tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       dtype=torch.float64)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m PRETRAINING:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[43mpretrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mpretrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m save_dir.mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m, parents=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrain_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrain_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAIN_DEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Stop if no improvement for 5 epochs\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Save final history\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, device, processor, num_epochs, save_path, scheduler, early_stopping_patience)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device, processor)\u001b[39m\n\u001b[32m     16\u001b[39m targets_tensor = targets_tensor.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m loss, loss_dict = criterion(predictions, targets_tensor)\n\u001b[32m     23\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mBLIP2MultiHeadRegression.forward\u001b[39m\u001b[34m(self, images, return_features)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# --- Q-Former processing ---\u001b[39;00m\n\u001b[32m     95\u001b[39m query_tokens = \u001b[38;5;28mself\u001b[39m.blip2.query_tokens.expand(images.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m).to(MAIN_DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m image_attention_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAIN_DEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m query_outputs = \u001b[38;5;28mself\u001b[39m.blip2.qformer(\n\u001b[32m     99\u001b[39m     query_embeds=query_tokens,\n\u001b[32m    100\u001b[39m     encoder_hidden_states=image_embeds,\n\u001b[32m    101\u001b[39m     encoder_attention_mask=image_attention_mask,\n\u001b[32m    102\u001b[39m     return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# --- Flatten Q-Former output ---\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def pretrain_model():\n",
    "    \"\"\"\n",
    "    Pretrain the model without style head.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"PRETRAINING MODE (no style head)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model setup\n",
    "    pretrain_model = BLIP2MultiHeadRegression(\n",
    "        blip2,\n",
    "        use_style_head=False,\n",
    "        train_qformer=True,\n",
    "        train_vision=False\n",
    "    )\n",
    "    load_model_from_latest(pretrain_model)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    pretrain_criterion = WeightedMultiHeadLoss(\n",
    "        movement_weight=1.0,\n",
    "        genre_weight=1.0,\n",
    "        use_style=False\n",
    "    ).to(MAIN_DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        pretrain_model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01  # Added weight decay for regularization\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (optional but recommended)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Save directory\n",
    "    save_dir = BASE_DIR / \"checkpoints\"\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Train\n",
    "    history = train_model(\n",
    "        model=pretrain_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=pretrain_criterion,\n",
    "        device=MAIN_DEVICE,\n",
    "        processor=processor,\n",
    "        num_epochs=5,\n",
    "        save_path=save_dir,\n",
    "        scheduler=scheduler,\n",
    "        early_stopping_patience=5  # Stop if no improvement for 5 epochs\n",
    "    )\n",
    "    \n",
    "    # Save final history\n",
    "    import json\n",
    "    history_path = save_dir / \"training_history.json\"\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"\\nTraining history saved to {history_path}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "if PRETRAINING:\n",
    "    pretrain_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019ab0a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import Blip2Processor\n",
    "from augmentation import augment_annotated_images\n",
    "\n",
    "# --- Global variables for lazy loading ---\n",
    "_model, _processor = None, None\n",
    "\n",
    "def initialize_model_for_webaccess():\n",
    "    \"\"\"\n",
    "    Initialize the BLIP2 multi-head regression model and processor.\n",
    "    Loads the latest checkpoint if available.\n",
    "    \"\"\"\n",
    "    model = BLIP2MultiHeadRegression(\n",
    "        blip2,\n",
    "        use_style_head=True,\n",
    "        train_qformer=True,\n",
    "        train_vision=False\n",
    "    )\n",
    "\n",
    "    load_model_from_latest(model)\n",
    "    model.eval()\n",
    "\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    return model, processor\n",
    "\n",
    "def get_model_and_processor():\n",
    "    \"\"\"\n",
    "    Lazy-load the model and processor.\n",
    "    \"\"\"\n",
    "    global _model, _processor\n",
    "    if _model is None or _processor is None:\n",
    "        _model, _processor = initialize_model_for_webaccess()\n",
    "        print(f\"Model and processor ready\")\n",
    "    return _model, _processor\n",
    "\n",
    "def forward_images(images):\n",
    "    model, processor = get_model_and_processor()\n",
    "    model.eval()\n",
    "\n",
    "    # Process all images as a batch\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    embeddings = outputs[\"combined\"].cpu().tolist()\n",
    "    print(f\"Forward pass completed on {len(images)} images\")\n",
    "    return embeddings\n",
    "\n",
    "def backward_single_image(image, target, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Perform a single training step on one image.\n",
    "    \"\"\"\n",
    "    model, processor = get_model_and_processor()\n",
    "    criterion = WeightedMultiHeadLoss(movement_weight=1.0, genre_weight=1.0, use_style=True).to(MAIN_DEVICE)\n",
    "\n",
    "    augmented_images, augmented_targets = augment_annotated_images([image], [target])\n",
    "    print(f\"Augmented to {len(augmented_images)} images for training\")\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "    inputs = processor(images=augmented_images, return_tensors=\"pt\").pixel_values.to(MAIN_DEVICE)\n",
    "    target_tensor = torch.tensor(augmented_targets, dtype=torch.float32).to(MAIN_DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    print(\"Model outputs obtained\", outputs.keys())\n",
    "    loss, loss_dict = criterion(outputs, target_tensor)\n",
    "\n",
    "    print(\"Backward pass with loss:\", loss.item(), loss_dict)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    return loss.item(), loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f568fa-494c-41a7-88e9-fd0953df2fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
