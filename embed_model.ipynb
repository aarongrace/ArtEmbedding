{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247e3479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on platform: PC\n"
     ]
    }
   ],
   "source": [
    "# determine platform from .env file\n",
    "try:\n",
    "    with open(\".env\", \"r\") as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split(\"=\")\n",
    "            if key == \"PLATFORM\":\n",
    "                PLATFORM = value\n",
    "    print(f\"Running on platform: {PLATFORM}\")\n",
    "except FileNotFoundError:\n",
    "    PLATFORM = \"PC\"\n",
    "    print(\"No .env file found. Defaulting to PC platform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c653d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision device: cpu, Main device: cuda, Prelim training: True\n",
      "Base directory: c:\\proggers\\ArtEmbedding\n"
     ]
    }
   ],
   "source": [
    "# configure global variables based on platform\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "if PLATFORM == \"PC\":\n",
    "    VISION_DEVICE = \"cpu\" # not enough GPU memory on laptop for full vision model\n",
    "    MAIN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    PRELIM_TRAINING = False # not prelim training on laptop\n",
    "\n",
    "elif PLATFORM == \"IDAS\":\n",
    "    if not torch.cuda.is_available():\n",
    "        raise EnvironmentError(\"CUDA is not available on IDAS. Please restart with GPU access.\")\n",
    "    VISION_DEVICE = \"cuda\"\n",
    "    MAIN_DEVICE = \"cuda\"\n",
    "    PRELIM_TRAINING = True\n",
    "else:\n",
    "    raise ValueError(f\"Unknown PLATFORM: {PLATFORM}\")\n",
    "\n",
    "print(f\"Vision device: {VISION_DEVICE}, Main device: {MAIN_DEVICE}, Prelim training: {PRELIM_TRAINING}\")\n",
    "BASE_DIR = Path.cwd()\n",
    "IMGS_DIR = BASE_DIR / \"paintings\"\n",
    "PRELIM_METADATA_DIR = BASE_DIR / \"metadata\" / \"paintings_metadata_with_rough_groundtruth.json\"\n",
    "CHECKPOINTS_DIR = BASE_DIR / \"checkpoints\"\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "# vector dimensions\n",
    "MOVEMENT_DIM = 6\n",
    "GENRE_DIM = 6\n",
    "STYLE_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac93d5ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4797419a6664e70bb721d9783d0453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model and processor loaded\n",
      "model sent to cpu\n"
     ]
    }
   ],
   "source": [
    "# load BLIP2 model and processor\n",
    "import os\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "model_name = \"Salesforce/blip2-flan-t5-xl\"\n",
    "local_model_path =  BASE_DIR / \"blip2_model\"\n",
    "\n",
    "if os.path.exists(local_model_path):\n",
    "    print(\"Loading model from local directory...\")\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(local_model_path)\n",
    "else:\n",
    "    print(\"Downloading model from Hugging Face...\")\n",
    "    processor = Blip2Processor.from_pretrained(model_name, use_fast=True)\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Save to local directory for future use\n",
    "    processor.save_pretrained(local_model_path)\n",
    "    blip2.save_pretrained(local_model_path)\n",
    "\n",
    "print(\"model and processor loaded\")\n",
    "blip2.to(VISION_DEVICE)  # Load model on CPU first if on computer\n",
    "print(f\"model sent to {VISION_DEVICE}\")\n",
    "\n",
    "# Freeze vision encoder to save memory; we are not training the vision encoder\n",
    "for param in blip2.vision_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec59fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint functions\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_latest_checkpoint():\n",
    "    checkpoint_files = glob.glob(os.path.join(CHECKPOINTS_DIR, \"model_*.pt\"))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    # Sort by modification time\n",
    "    checkpoint_files.sort(key=os.path.getmtime)\n",
    "    return checkpoint_files[-1]\n",
    "\n",
    "# as we are not training the vision model, load only the relevant parts\n",
    "def load_model_from_latest(model):\n",
    "    latest_check_point = get_latest_checkpoint()\n",
    "    if latest_check_point is None:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return\n",
    "    state_dict = torch.load(latest_check_point, map_location='cpu')\n",
    "    \n",
    "    model.shared_features.load_state_dict(state_dict[\"shared_features\"])\n",
    "    model.movement_head.load_state_dict(state_dict[\"movement_head\"])\n",
    "    model.genre_head.load_state_dict(state_dict[\"genre_head\"])\n",
    "    model.style_head.load_state_dict(state_dict[\"style_head\"])\n",
    "    \n",
    "    if \"qformer\" in state_dict:\n",
    "        model.blip2.qformer.load_state_dict(state_dict[\"qformer\"])\n",
    "        print(\" Loaded Q-Former weights\")\n",
    "    \n",
    "    print(f\" Loaded weights from {latest_check_point}\")\n",
    "\n",
    "\n",
    "def save_progress(model, file_name):\n",
    "    checkpoint_file = os.path.join(CHECKPOINTS_DIR, f\"{file_name}.pt\")\n",
    "\n",
    "    state_dict = {\n",
    "        \"shared_features\": model.shared_features.state_dict(),\n",
    "        \"movement_head\": model.movement_head.state_dict(),\n",
    "        \"genre_head\": model.genre_head.state_dict(),\n",
    "        \"style_head\": model.style_head.state_dict(),\n",
    "    }\n",
    "    # Optionally include Q-Former if it's being trained\n",
    "    if any(p.requires_grad for p in model.blip2.qformer.parameters()):\n",
    "        state_dict[\"qformer\"] = model.blip2.qformer.state_dict()\n",
    "        \n",
    "    torch.save(state_dict, checkpoint_file)\n",
    "    print(f\" Saved fine-tuned modules to: {checkpoint_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962f8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load/create persistent train/test split\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "def get_split( total_length: int, test_percentage: float = 0.2, \n",
    "              split_file: str = BASE_DIR/\"data_split.json\", seed: int = 42,):\n",
    "    \"\"\"\n",
    "    Create or load a consistent train/test split for a dataset of a given size.\n",
    "    A split is saved/loaded based on the total_length of the dataset.\n",
    "\n",
    "    Args:\n",
    "        total_length (int): Total number of samples/images.\n",
    "        test_percentage (float): Fraction of samples to use for testing.\n",
    "        split_file (str): JSON file path to store splits.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        (train_indices, test_indices): Two lists of indices.\n",
    "    \"\"\"\n",
    "    if os.path.exists(split_file):\n",
    "        with open(split_file, \"r\") as f:\n",
    "            all_splits = json.load(f)\n",
    "    else:\n",
    "        all_splits = {}\n",
    "\n",
    "    key = str(total_length)\n",
    "\n",
    "    if key in all_splits:\n",
    "        split_data = all_splits[key]\n",
    "        print(f\"Loaded existing split for length {total_length}: \"\n",
    "              f\"{len(split_data['train'])} train, {len(split_data['test'])} test\")\n",
    "        return split_data[\"train\"], split_data[\"test\"]\n",
    "\n",
    "    # Otherwise, generate a new split\n",
    "    random.seed(seed)\n",
    "    indices = list(range(total_length))\n",
    "    random.shuffle(indices)\n",
    "    num_test = int(total_length * test_percentage)\n",
    "    test_indices = indices[:num_test]\n",
    "    train_indices = indices[num_test:]\n",
    "\n",
    "    all_splits[key] = {\n",
    "        \"train\": train_indices,\n",
    "        \"test\": test_indices\n",
    "    }\n",
    "    with open(split_file, \"w\") as f:\n",
    "        json.dump(all_splits, f, indent=2)\n",
    "\n",
    "    print(f\"Created new split for length {total_length}: \"\n",
    "          f\"{len(train_indices)} train, {len(test_indices)} test â†’ saved to {split_file}\")\n",
    "    return train_indices, test_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a494bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ LOADING DATASET ================================================================================\n",
      "Found 30922 image files in c:\\proggers\\ArtEmbedding\\paintings\n",
      "Loaded metadata for 29995 paintings\n",
      "Matched 29995/30922 images with valid targets\n",
      "Loaded existing split for length 29995: 26996 train, 2999 test\n",
      "\n",
      "Train: 26996 images, Test: 2999 images\n"
     ]
    }
   ],
   "source": [
    "# dataloader creation for preliminary training; lazy loading is neeeded due to dataset size\n",
    "# only necessary for preliminary training, as FastAPI backend provides experted annotated data\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PaintingDataset(Dataset):\n",
    "    def __init__(self, image_paths, targets):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        target = self.targets[idx]\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        return path, target_tensor\n",
    "\n",
    "# need a collate function because torch doesn't work well with lists of strings\n",
    "def collate_fn(batch):\n",
    "    paths, targets = zip(*batch)\n",
    "    targets_tensor = torch.stack(targets)\n",
    "    return list(paths), targets_tensor\n",
    "\n",
    "\n",
    "def create_train_test_loaders( imgs_directory_path, pretraining_metadata_path, \n",
    "                              batch_size_train=32, batch_size_test=32, test_percentage=0.1):\n",
    "    print(\"=\"*80, \"LOADING DATASET\", \"=\"*80)\n",
    "    image_paths = []\n",
    "    image_ids = []\n",
    "    all_files = sorted(os.listdir(imgs_directory_path))\n",
    "    \n",
    "    for file_name in all_files:\n",
    "        if file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            path = os.path.join(imgs_directory_path, file_name)\n",
    "            image_paths.append(path)\n",
    "            # Extract ID from filename (first part before underscore)\n",
    "            image_ids.append(file_name.split(\"_\")[0])\n",
    "    print(f\"Found {len(image_paths)} image files in {imgs_directory_path}\")\n",
    "    \n",
    "    with open(pretraining_metadata_path, 'r', encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"Loaded metadata for {len(metadata)} paintings\")\n",
    "    \n",
    "    targets = []\n",
    "    valid_paths = []\n",
    "    matched_count = 0\n",
    "    for path, img_id in zip(image_paths, image_ids):\n",
    "        if img_id in metadata and \"rough_groundtruth\" in metadata[img_id]:\n",
    "            target = metadata[img_id][\"rough_groundtruth\"]\n",
    "            targets.append(target)\n",
    "            valid_paths.append(path)\n",
    "            matched_count += 1\n",
    "    if matched_count == 0:\n",
    "        raise ValueError(\"No images matched with metadata! Check your image IDs and metadata format.\")\n",
    "    else:\n",
    "        print(f\"Matched {matched_count}/{len(image_paths)} images with valid targets\")\n",
    "\n",
    "    # ensuring all targets have the same dimension\n",
    "    first_target = targets[0]\n",
    "    target_dim = len(first_target)\n",
    "    for i, t in enumerate(targets[:3]):\n",
    "        if len(t) != target_dim:\n",
    "            raise ValueError(f\"Inconsistent target dimensions: image {i} has {len(t)}, expected {target_dim}\")\n",
    "    \n",
    "    # split into train and test sets\n",
    "    num_images = len(valid_paths)\n",
    "    train_indices, test_indices = get_split(total_length=num_images, test_percentage=test_percentage)\n",
    "\n",
    "    train_paths = [valid_paths[i] for i in train_indices]\n",
    "    train_targets = [targets[i] for i in train_indices]\n",
    "    test_paths = [valid_paths[i] for i in test_indices]\n",
    "    test_targets = [targets[i] for i in test_indices]\n",
    "    print(f\"\\nTrain: {len(train_paths)} images, Test: {len(test_paths)} images\")\n",
    "    \n",
    "    train_dataset = PaintingDataset(train_paths, train_targets)\n",
    "    test_dataset = PaintingDataset(test_paths, test_targets)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size_test,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "if PRELIM_TRAINING:\n",
    "    train_loader, test_loader = create_train_test_loaders(\n",
    "        imgs_directory_path=IMGS_DIR,\n",
    "        pretraining_metadata_path=PRELIM_METADATA_DIR,\n",
    "        batch_size_train=32,\n",
    "        batch_size_test=32,\n",
    "        test_percentage=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6be0459",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# GPU memory monitoring utility\n",
    "def print_gpu_mem(prefix=\"GPU\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2   # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2     # MB\n",
    "        print(f\"{prefix} Memory â€” Allocated: {allocated:.2f} MB | Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766b12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head regression model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BLIP2MultiHeadRegression(nn.Module):\n",
    "    def __init__(self, blip2_model,\n",
    "                 use_style_head=True,\n",
    "                 train_qformer=False,\n",
    "                 train_vision=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Core model ---\n",
    "        self.blip2 = blip2_model\n",
    "        self.use_style_head = use_style_head\n",
    "\n",
    "        # --- Control what's trainable ---\n",
    "        for param in self.blip2.vision_model.parameters():\n",
    "            param.requires_grad = train_vision\n",
    "        for param in self.blip2.qformer.parameters():\n",
    "            param.requires_grad = train_qformer\n",
    "\n",
    "        # --- Move modules to appropriate devices ---\n",
    "        self.blip2.vision_model.to(VISION_DEVICE)\n",
    "        self.blip2.qformer.to(MAIN_DEVICE)\n",
    "\n",
    "        # query_tokens is an nn.Parameter â†’ rewrap properly after moving\n",
    "        self.blip2.query_tokens = nn.Parameter(\n",
    "            self.blip2.query_tokens.to(MAIN_DEVICE)\n",
    "        )\n",
    "\n",
    "        # --- Config info ---\n",
    "        num_query_tokens = blip2_model.config.num_query_tokens\n",
    "        hidden_size = blip2_model.config.qformer_config.hidden_size\n",
    "        feature_dim = num_query_tokens * hidden_size\n",
    "\n",
    "        print(f\"Num query tokens: {num_query_tokens}\")\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        print(f\"Feature dim: {feature_dim}\")\n",
    "        print(f\"Use style head: {use_style_head}\")\n",
    "\n",
    "        # --- Shared feature extraction ---\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "        # --- Movement and Genre heads ---\n",
    "        self.movement_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, MOVEMENT_DIM)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "        self.genre_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, GENRE_DIM)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "        # --- Style head (always defined, but only used if enabled) ---\n",
    "        self.style_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, STYLE_DIM)\n",
    "        ).to(MAIN_DEVICE)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # --- Vision encoding ---\n",
    "        images_vision = images.to(VISION_DEVICE)\n",
    "\n",
    "        if self.training and next(self.blip2.vision_model.parameters()).requires_grad:\n",
    "            vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "\n",
    "        image_embeds = vision_outputs.last_hidden_state.to(MAIN_DEVICE)  # move to GPU\n",
    "\n",
    "        # --- Q-Former processing ---\n",
    "        query_tokens = self.blip2.query_tokens.expand(images.shape[0], -1, -1).to(MAIN_DEVICE)\n",
    "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long).to(MAIN_DEVICE)\n",
    "\n",
    "        query_outputs = self.blip2.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # --- Flatten Q-Former output ---\n",
    "        query_hidden_states = query_outputs.last_hidden_state\n",
    "        flattened = query_hidden_states.flatten(start_dim=1)\n",
    "\n",
    "        # --- Shared features ---\n",
    "        shared_features = self.shared_features(flattened)\n",
    "\n",
    "        # --- Regression heads ---\n",
    "        movement_scores = self.movement_head(shared_features)\n",
    "        genre_scores = self.genre_head(shared_features)\n",
    "        style_scores = self.style_head(shared_features)\n",
    "\n",
    "        outputs = {\n",
    "            'movement': movement_scores,\n",
    "            'genre': genre_scores,\n",
    "            'style': style_scores,\n",
    "        }\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa2b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head weighted loss function\n",
    "class WeightedMultiHeadLoss(nn.Module):\n",
    "    def __init__(self, movement_weight=1.0, genre_weight=1.0, style_weight=1.0, use_style=True):\n",
    "        super().__init__()\n",
    "        self.movement_weight = movement_weight\n",
    "        self.genre_weight = genre_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.use_style = use_style\n",
    "\n",
    "        # MSE loss for continuous targets\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, predictions, targets, confidences=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: dict with 'movement', 'genre', 'style' (raw outputs)\n",
    "            targets: tensor [batch, total_dim] (target values)\n",
    "            confidences: dict with confidence scores (optional)\n",
    "        \"\"\"\n",
    "        # Split targets by head dimensions\n",
    "        movement_target = targets[:, :MOVEMENT_DIM]\n",
    "        genre_target    = targets[:, MOVEMENT_DIM : MOVEMENT_DIM + GENRE_DIM]\n",
    "\n",
    "        # --- Movement loss ---\n",
    "        movement_loss = self.mse(predictions['movement'], movement_target)\n",
    "        if confidences is not None and 'movement' in confidences:\n",
    "            movement_loss = movement_loss * confidences['movement']\n",
    "        movement_loss = movement_loss.mean() * self.movement_weight\n",
    "\n",
    "        # --- Genre loss ---\n",
    "        genre_loss = self.mse(predictions['genre'], genre_target)\n",
    "        if confidences is not None and 'genre' in confidences:\n",
    "            genre_loss = genre_loss * confidences['genre']\n",
    "        genre_loss = genre_loss.mean() * self.genre_weight\n",
    "\n",
    "        # --- Total loss ---\n",
    "        total_loss = movement_loss + genre_loss\n",
    "        loss_dict = {'movement': movement_loss.item(), 'genre': genre_loss.item()}\n",
    "\n",
    "        # --- Style loss (optional) ---\n",
    "        if self.use_style:\n",
    "            style_target = targets[:, MOVEMENT_DIM + GENRE_DIM :]\n",
    "            style_loss = self.mse(predictions['style'], style_target)\n",
    "            if confidences is not None and 'style' in confidences:\n",
    "                style_loss = style_loss * confidences['style']\n",
    "            style_loss = style_loss.mean() * self.style_weight\n",
    "            total_loss += style_loss\n",
    "            loss_dict['style'] = style_loss.item()\n",
    "\n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4478b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation function for preliminary training\n",
    "from PIL import Image, ImageOps, UnidentifiedImageError, ImageFile\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Do NOT allow truncated images - raise errors instead\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "\n",
    "def augment_batch(image_paths, targets, processor):\n",
    "    \"\"\"\n",
    "    Load images, create flipped versions, and return pixel values.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of file paths to images (length batch_size)\n",
    "        targets: Tensor of shape [batch_size, 12]\n",
    "        processor: BLIP2 processor\n",
    "        \n",
    "    Returns:\n",
    "        pixel_values: Tensor of shape [batch_size*2, 3, H, W]\n",
    "        doubled_targets: Tensor of shape [batch_size*2, 12]\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    doubled_targets = []\n",
    "    \n",
    "    # Iterate by index since targets is a tensor\n",
    "    for idx in range(len(image_paths)):\n",
    "        img_path = image_paths[idx]\n",
    "        target = targets[idx]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except (OSError, UnidentifiedImageError) as e:\n",
    "            print(f\"Skipping corrupted image: {img_path} ({e})\")\n",
    "            continue\n",
    "        \n",
    "        # Add original image\n",
    "        images.append(img)\n",
    "        doubled_targets.append(target)\n",
    "        \n",
    "        # Add horizontally flipped image\n",
    "        flipped_img = ImageOps.mirror(img)\n",
    "        images.append(flipped_img)\n",
    "        doubled_targets.append(target)\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Process all images at once with processor\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    # Stack all target rows into [num_images, 12]\n",
    "    targets_tensor = torch.stack(doubled_targets)\n",
    "    \n",
    "    return pixel_values, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac3e6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing epoch functions\n",
    "import time\n",
    "def test_epoch(model, dataloader, criterion, device, processor):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image_paths, targets in dataloader:\n",
    "            # Augment the batch (same as training for consistency)\n",
    "            pixel_values, targets_tensor = augment_batch(image_paths, targets, processor)\n",
    "            \n",
    "            pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "            targets_tensor = targets_tensor.to(device, non_blocking=True)\n",
    "            \n",
    "            predictions = model(pixel_values)\n",
    "            loss, _ = criterion(predictions, targets_tensor)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation complete | Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, processor, val_loader=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    images_processed = 0\n",
    "    for step, (image_paths, targets) in enumerate(dataloader):\n",
    "        # print(f\"step: {step}, image_paths: {image_paths}, targets: {targets}\")\n",
    "        # Augment the batch\n",
    "        pixel_values, targets_tensor = augment_batch(image_paths, targets, processor)\n",
    "        if pixel_values == None:\n",
    "            continue\n",
    "        \n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        targets_tensor = targets_tensor.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values)\n",
    "        loss, loss_dict = criterion(predictions, targets_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        time_elapsed = time.time() - start_time\n",
    "\n",
    "        images_processed += len(pixel_values)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step + 1}/{len(dataloader)} | Images: {images_processed} | Time: {time_elapsed:.2f}s | Loss: {loss.item():.4f}\")\n",
    "            # print_gpu_mem()\n",
    "        \n",
    "        # the data size is large enough that we want to validate more than once per epoch to prevent overfitting\n",
    "        if val_loader is not None and (step + 1) % 100 == 0:\n",
    "            val_loss = test_epoch(model, val_loader, criterion, device, processor)\n",
    "            print(f\" Validation Loss after {step + 1} steps: {val_loss:.4f}\")\n",
    "        \n",
    "    num_batches = len(dataloader)\n",
    "    total_images = num_batches * dataloader.batch_size * 2  # *2 for augmentation\n",
    "    avg_loss = total_loss / num_batches\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch complete | Avg Loss: {avg_loss:.4f} | Total images: {total_images} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "213877d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================PRELIMINARY TRAINING============================================================\n",
      "Num query tokens: 32\n",
      "Hidden size: 768\n",
      "Feature dim: 24576\n",
      "Use style head: False\n",
      " Loaded Q-Former weights\n",
      " Loaded weights from c:\\proggers\\ArtEmbedding\\checkpoints\\model_20251017_160438.pt\n",
      "\n",
      "============================================================\n",
      "Epoch 1/5\n",
      "Learning rate: 1.00e-05\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m PRELIM_TRAINING:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     history = \u001b[43mrun_preliminary_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblip2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mrun_preliminary_training\u001b[39m\u001b[34m(blip2, processor, train_loader, test_loader, num_epochs, early_stopping_patience)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAIN_DEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# --- Validation ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device, processor, val_loader)\u001b[39m\n\u001b[32m     37\u001b[39m targets_tensor = targets_tensor.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     40\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m loss, loss_dict = criterion(predictions, targets_tensor)\n\u001b[32m     44\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mBLIP2MultiHeadRegression.forward\u001b[39m\u001b[34m(self, images)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         vision_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblip2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages_vision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m image_embeds = vision_outputs.last_hidden_state.to(MAIN_DEVICE)  \u001b[38;5;66;03m# move to GPU\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --- Q-Former processing ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:511\u001b[39m, in \u001b[36mBlip2VisionModel.forward\u001b[39m\u001b[34m(self, pixel_values, interpolate_pos_encoding, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify pixel_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m last_hidden_state = encoder_outputs.last_hidden_state\n\u001b[32m    517\u001b[39m last_hidden_state = \u001b[38;5;28mself\u001b[39m.post_layernorm(last_hidden_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:468\u001b[39m, in \u001b[36mBlip2Encoder.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, **kwargs)\u001b[39m\n\u001b[32m    466\u001b[39m hidden_states = inputs_embeds\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m encoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     hidden_states = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(last_hidden_state=hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:386\u001b[39m, in \u001b[36mBlip2EncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m residual = hidden_states\n\u001b[32m    385\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m hidden_states = hidden_states + residual\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:352\u001b[39m, in \u001b[36mBlip2MLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(hidden_states)\n\u001b[32m    354\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.fc2(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\omaha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# preliminary training function\n",
    "import torch, json\n",
    "def run_preliminary_training(\n",
    "    blip2,\n",
    "    processor,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs=5,\n",
    "    early_stopping_patience=2\n",
    "):\n",
    "    print(\"=\"*60 + \"PRELIMINARY TRAINING\" + \"=\"*60)\n",
    "\n",
    "    # ---------------- Model setup ----------------\n",
    "    model = BLIP2MultiHeadRegression(\n",
    "        blip2,\n",
    "        use_style_head=False,\n",
    "        train_qformer=True,\n",
    "        train_vision=False\n",
    "    )\n",
    "    load_model_from_latest(model)\n",
    "\n",
    "    criterion = WeightedMultiHeadLoss(\n",
    "        movement_weight=1.0,\n",
    "        genre_weight=1.0,\n",
    "        use_style=False\n",
    "    ).to(MAIN_DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # ---------------- Training loop ----------------\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"learning_rates\": []}\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learning rate: {current_lr:.2e}\")\n",
    "        history[\"learning_rates\"].append(current_lr)\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, MAIN_DEVICE, processor)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_loss = test_epoch(model, test_loader, criterion, MAIN_DEVICE, processor)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # --- Improvement tracking ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        if early_stopping_patience and epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
    "            print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        # --- Scheduler update ---\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # --- Save progress ---\n",
    "        file_name = f\"model_epoch_{epoch}_valLoss_{val_loss:.4f}\"\n",
    "        save_progress(model, file_name)\n",
    "\n",
    "    # ---------------- Wrap up ----------------\n",
    "    print(\"\\n\" + \"=\"*30, \"TRAINING COMPLETE\", \"=\"*30)\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Save history\n",
    "    history_path = CHECKPOINTS_DIR / \"training_history.json\"\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"\\nTraining history saved to {history_path}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "if PRELIM_TRAINING:\n",
    "    history = run_preliminary_training(\n",
    "        blip2,\n",
    "        processor,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        num_epochs=5,\n",
    "        early_stopping_patience=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019ab0a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# web access functions\n",
    "import torch\n",
    "from transformers import Blip2Processor\n",
    "from augmentation import augment_annotated_images\n",
    "\n",
    "# --- Global variables for lazy loading ---\n",
    "_model, _processor = None, None\n",
    "\n",
    "def initialize_model_for_webaccess():\n",
    "    \"\"\"\n",
    "    Initialize the BLIP2 multi-head regression model and processor.\n",
    "    Loads the latest checkpoint if available.\n",
    "    \"\"\"\n",
    "    model = BLIP2MultiHeadRegression(\n",
    "        blip2,\n",
    "        use_style_head=True,\n",
    "        train_qformer=True,\n",
    "        train_vision=False\n",
    "    )\n",
    "\n",
    "    load_model_from_latest(model)\n",
    "    model.eval()\n",
    "\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    return model, processor\n",
    "\n",
    "def get_model_and_processor():\n",
    "    \"\"\"\n",
    "    Lazy-load the model and processor.\n",
    "    \"\"\"\n",
    "    global _model, _processor\n",
    "    if _model is None or _processor is None:\n",
    "        _model, _processor = initialize_model_for_webaccess()\n",
    "        print(f\"Model and processor ready\")\n",
    "    return _model, _processor\n",
    "\n",
    "def forward_images(images):\n",
    "    model, processor = get_model_and_processor()\n",
    "    model.eval()\n",
    "\n",
    "    # Process all images as a batch\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    print(f\"Outputs: movement -> {outputs['movement'].shape}, genre -> {outputs['genre'].shape}, style -> {outputs['style'].shape}\")\n",
    "    # Move each head to CPU and convert to list\n",
    "    embeddings = torch.cat([ outputs['movement'], outputs['genre'], outputs['style'] ], \n",
    "                           dim=1).cpu().tolist()\n",
    "    \n",
    "\n",
    "    print(f\"Forward pass completed on {len(images)} images\")\n",
    "    return embeddings\n",
    "\n",
    "def backward_single_image(image, target, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Perform a single training step on one image.\n",
    "    \"\"\"\n",
    "    model, processor = get_model_and_processor()\n",
    "    criterion = WeightedMultiHeadLoss(movement_weight=1.0, genre_weight=1.0, use_style=True).to(MAIN_DEVICE)\n",
    "\n",
    "    augmented_images, augmented_targets = augment_annotated_images([image], [target])\n",
    "    print(f\"Augmented to {len(augmented_images)} images for training\")\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "    inputs = processor(images=augmented_images, return_tensors=\"pt\").pixel_values.to(MAIN_DEVICE)\n",
    "    target_tensor = torch.tensor(augmented_targets, dtype=torch.float32).to(MAIN_DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    print(\"Model outputs obtained\", outputs.keys())\n",
    "    loss, loss_dict = criterion(outputs, target_tensor)\n",
    "\n",
    "    print(\"Backward pass with loss:\", loss.item(), loss_dict)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    return loss.item(), loss_dict"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
