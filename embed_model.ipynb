{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e083fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MOVEMENT_DIM = 5\n",
    "GENRE_DIM = 5\n",
    "STYLE_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c653d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running from IDAS, probably\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# get the images\n",
    "from PIL import Image\n",
    "import os, json, torch\n",
    "\n",
    "\n",
    "try:  # Check if running in Colab\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"running in Google Colab\")\n",
    "    mount_path = '/content/drive'\n",
    "    if not os.path.exists(mount_path):\n",
    "        drive.mount(mount_path)\n",
    "    imgs_directory_path = '/content/drive/MyDrive/ArtEmbed'\n",
    "    pretraining_metadata = '/content/drive/MyDrive/ArtEmbed/wikiart_metadata_with_pretraining_groundtruth.json'\n",
    "\n",
    "except ImportError:  # Not Colab\n",
    "    from pathlib import Path\n",
    "    IN_COLAB = False\n",
    "\n",
    "    try:\n",
    "        BASE_DIR = Path(__file__).resolve().parent  # works in scripts\n",
    "        print(\"running from laptop, probably\")\n",
    "    except NameError:\n",
    "        BASE_DIR = Path.cwd()  # fallback for notebooks\n",
    "        print(\"running from IDAS, probably\")\n",
    "\n",
    "    imgs_directory_path = BASE_DIR / \"paintings\"\n",
    "    pretraining_metadata = BASE_DIR / \"metadata\" / \"wikiart_metadata_with_pretraining_groundtruth.json\"\n",
    "\n",
    "\n",
    "def load_image_from_drive():\n",
    "  image_array = []\n",
    "  image_names = []\n",
    "  image_ids =[]\n",
    "\n",
    "  all_files = sorted(os.listdir(imgs_directory_path))\n",
    "  for file_name in all_files:\n",
    "      if file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "          path = os.path.join(imgs_directory_path, file_name)\n",
    "          img = Image.open(path).convert(\"RGB\")\n",
    "          image_array.append(img)\n",
    "          image_names.append(file_name)\n",
    "          image_ids.append(file_name.split(\"_\")[0])\n",
    "\n",
    "  print(f\"Found {len(image_array)} images. Image ids: {image_ids}\")\n",
    "  return image_array, image_ids\n",
    "\n",
    "def load_pretraining_metadata():\n",
    "    with open(pretraining_metadata, 'r', encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    # print(metadata.keys())\n",
    "    print(f\"Found metadata for {len(metadata)} paintings.\")\n",
    "    return metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93d5ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ywang852/.local/lib/python3.11/site-packages (4.57.0)\n",
      "Requirement already satisfied: accelerate in /home/ywang852/.local/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ywang852/.local/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /home/ywang852/.local/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ywang852/.local/lib/python3.11/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ywang852/.local/lib/python3.11/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/ywang852/.local/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ywang852/.local/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ywang852/.local/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ywang852/.local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ywang852/.local/lib/python3.11/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ywang852/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ywang852/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (65.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ywang852/.local/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ywang852/.local/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ywang852/.local/lib/python3.11/site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ywang852/hpchome/ArtEmbedding/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory...\n",
      "Processor loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Loaded model on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Import libraries ---\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# --- Load BLIP-2 model and processor ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Salesforce/blip2-flan-t5-xl\"\n",
    "local_model_path =  BASE_DIR / \"blip2_model\"\n",
    "\n",
    "if os.path.exists(local_model_path):\n",
    "    print(\"Loading model from local directory...\")\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    print(\"Processor loaded\")\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(local_model_path)\n",
    "    print(\"Model loaded\")\n",
    "else:\n",
    "    print(\"Downloading model from Hugging Face...\")\n",
    "    processor = Blip2Processor.from_pretrained(model_name, use_fast=True)\n",
    "    blip2 = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Save to local directory for future use\n",
    "    processor.save_pretrained(local_model_path)\n",
    "    blip2.save_pretrained(local_model_path)\n",
    "\n",
    "blip2.to(\"cpu\")  # Load model on CPU to avoid GPU memory issues\n",
    "print(f\"Loaded model on cpu\")\n",
    "\n",
    "# Freeze vision encoder to save memory; we are not training the vision encoder\n",
    "for param in blip2.vision_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95070a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_dataloader(image_list, target_list, processor, device, batch_size=4, shuffle=True):\n",
    "    # Convert images to pixel values tensors\n",
    "    pixel_values_tensor = torch.stack([\n",
    "        processor(images=img, return_tensors=\"pt\").pixel_values.squeeze(0) \n",
    "        for img in image_list\n",
    "    ])  # [N, 3, H, W]\n",
    "\n",
    "    # Convert targets to tensor\n",
    "    targets_tensor = torch.stack([torch.tensor(t, dtype=torch.float32) for t in target_list])  # [N, total_dims]\n",
    "\n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(pixel_values_tensor, targets_tensor)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Wrap batches with device transfer\n",
    "    def device_loader():\n",
    "        for batch_pixel_values, batch_targets in dataloader:\n",
    "            yield batch_pixel_values.to(device, non_blocking=True), batch_targets.to(device, non_blocking=True)\n",
    "\n",
    "    print(f\"Created DataLoader with {len(dataloader)} batches of size {batch_size}\")\n",
    "\n",
    "    return device_loader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be0459",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_gpu_mem(prefix=\"GPU\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2   # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2     # MB\n",
    "        print(f\"{prefix} Memory — Allocated: {allocated:.2f} MB | Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e21e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BLIP2MultiHeadRegression(nn.Module):\n",
    "    def __init__(self, blip2_model,\n",
    "                 use_style_head=True,\n",
    "                 train_qformer=False,\n",
    "                 train_vision=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Devices ---\n",
    "        self.main_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.vision_device = torch.device(\"cpu\")  # keep heavy vision encoder on CPU\n",
    "\n",
    "        # --- Core model ---\n",
    "        self.blip2 = blip2_model\n",
    "        self.use_style_head = use_style_head\n",
    "\n",
    "        # --- Control what's trainable ---\n",
    "        for param in self.blip2.vision_model.parameters():\n",
    "            param.requires_grad = train_vision\n",
    "        for param in self.blip2.qformer.parameters():\n",
    "            param.requires_grad = train_qformer\n",
    "\n",
    "        # --- Move modules to appropriate devices ---\n",
    "        self.blip2.vision_model.to(self.vision_device)\n",
    "        self.blip2.qformer.to(self.main_device)\n",
    "\n",
    "        # query_tokens is an nn.Parameter → rewrap properly after moving\n",
    "        self.blip2.query_tokens = nn.Parameter(\n",
    "            self.blip2.query_tokens.to(self.main_device)\n",
    "        )\n",
    "\n",
    "        # --- Config info ---\n",
    "        num_query_tokens = blip2_model.config.num_query_tokens\n",
    "        hidden_size = blip2_model.config.qformer_config.hidden_size\n",
    "        feature_dim = num_query_tokens * hidden_size\n",
    "\n",
    "        print(f\"Num query tokens: {num_query_tokens}\")\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        print(f\"Feature dim: {feature_dim}\")\n",
    "        print(f\"Use style head: {use_style_head}\")\n",
    "        print(f\"Vision model device: {self.vision_device}\")\n",
    "        print(f\"Q-Former device: {self.main_device}\")\n",
    "\n",
    "        # --- Shared feature extraction ---\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "        # --- Movement and Genre heads ---\n",
    "        self.movement_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, MOVEMENT_DIM)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "        self.genre_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, GENRE_DIM)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "        # --- Style head (always defined, but only used if enabled) ---\n",
    "        self.style_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, STYLE_DIM)\n",
    "        ).to(self.main_device)\n",
    "\n",
    "    def forward(self, images, return_features=False):\n",
    "        \"\"\"\n",
    "        Forward pass with optional CPU/GPU split for vision model.\n",
    "\n",
    "        Args:\n",
    "            images: [batch_size, 3, H, W]\n",
    "            return_features: If True, also return shared features\n",
    "\n",
    "        Returns:\n",
    "            dict with keys: 'movement', 'genre', 'style', 'combined', optionally 'features'\n",
    "        \"\"\"\n",
    "        device = next(self.shared_features.parameters()).device  # GPU for rest of model\n",
    "\n",
    "        # --- Vision encoding ---\n",
    "        vision_device = next(self.blip2.vision_model.parameters()).device\n",
    "        images_vision = images.to(vision_device)\n",
    "\n",
    "        if self.training and next(self.blip2.vision_model.parameters()).requires_grad:\n",
    "            vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vision_outputs = self.blip2.vision_model(pixel_values=images_vision)\n",
    "\n",
    "        image_embeds = vision_outputs.last_hidden_state.to(device)  # move to GPU\n",
    "\n",
    "        # --- Q-Former processing ---\n",
    "        query_tokens = self.blip2.query_tokens.expand(images.shape[0], -1, -1).to(device)\n",
    "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "        query_outputs = self.blip2.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # --- Flatten Q-Former output ---\n",
    "        query_hidden_states = query_outputs.last_hidden_state\n",
    "        flattened = query_hidden_states.flatten(start_dim=1)\n",
    "\n",
    "        # --- Shared features ---\n",
    "        shared_features = self.shared_features(flattened)\n",
    "\n",
    "        # --- Regression heads ---\n",
    "        movement_scores = torch.sigmoid(self.movement_head(shared_features))\n",
    "        genre_scores = torch.sigmoid(self.genre_head(shared_features))\n",
    "        style_scores = torch.sigmoid(self.style_head(shared_features))\n",
    "\n",
    "        outputs = {\n",
    "            'movement': movement_scores,\n",
    "            'genre': genre_scores,\n",
    "            'style': style_scores,\n",
    "            'combined': torch.cat([movement_scores, genre_scores, style_scores], dim=1)\n",
    "        }\n",
    "\n",
    "        if return_features:\n",
    "            outputs['features'] = shared_features\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class WeightedMultiHeadLoss(nn.Module):\n",
    "    def __init__(self, movement_weight=1.0, genre_weight=0.7, style_weight=0.8, use_style=True):\n",
    "        super().__init__()\n",
    "        self.movement_weight = movement_weight\n",
    "        self.genre_weight = genre_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.use_style = use_style\n",
    "\n",
    "    def forward(self, predictions, targets, confidences=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: dict with 'movement', 'genre', 'style'\n",
    "            targets: tensor [batch, total_dim] (already prepared)\n",
    "            confidences: dict with confidence scores (optional)\n",
    "        \"\"\"\n",
    "        # Split targets using global dims\n",
    "        movement_target = targets[:, :MOVEMENT_DIM]\n",
    "        genre_target    = targets[:, MOVEMENT_DIM : MOVEMENT_DIM + GENRE_DIM]\n",
    "        style_target    = targets[:, MOVEMENT_DIM + GENRE_DIM :]\n",
    "\n",
    "        mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "        # Movement loss\n",
    "        movement_loss = mse(predictions['movement'], movement_target)\n",
    "        if confidences is not None and 'movement' in confidences:\n",
    "            movement_loss = movement_loss * confidences['movement']\n",
    "        movement_loss = movement_loss.mean() * self.movement_weight\n",
    "\n",
    "        # Genre loss\n",
    "        genre_loss = mse(predictions['genre'], genre_target)\n",
    "        if confidences is not None and 'genre' in confidences:\n",
    "            genre_loss = genre_loss * confidences['genre']\n",
    "        genre_loss = genre_loss.mean() * self.genre_weight\n",
    "\n",
    "        total_loss = movement_loss + genre_loss\n",
    "        loss_dict = {'movement': movement_loss.item(), 'genre': genre_loss.item()}\n",
    "\n",
    "        # Style loss\n",
    "        if self.use_style:\n",
    "            style_loss = mse(predictions['style'], style_target)\n",
    "            if confidences is not None and 'style' in confidences:\n",
    "                style_loss = style_loss * confidences['style']\n",
    "            style_loss = style_loss.mean() * self.style_weight\n",
    "            total_loss += style_loss\n",
    "            loss_dict['style'] = style_loss.item()\n",
    "\n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033942c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    num_images = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(dataloader):\n",
    "        batch_size = pixel_values.size(0)\n",
    "        num_batches += 1\n",
    "        num_images += batch_size\n",
    "\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)  # [batch, total_dim]\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values)\n",
    "        loss, loss_dict = criterion(predictions, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        time_elapsed = time.time() - start_time\n",
    "        # if step % 2 == 0:\n",
    "        if True:\n",
    "            print(f\"Step {step} image number {num_images} time_elapsed {time_elapsed:.2f}s | Loss: {loss.item():.4f}\")\n",
    "            print_gpu_mem()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch complete | Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Time: {epoch_time:.2f}s | Per batch: {epoch_time/num_batches:.2f}s | Per image: {epoch_time/num_images:.4f}s\")\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def test_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pixel_values, targets in dataloader:\n",
    "            pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)  # [batch, total_dim]\n",
    "\n",
    "            predictions = model(pixel_values)\n",
    "            loss, _ = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation complete | Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7d3d5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10, save_path=None):\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            val_loss = test_epoch(model, val_loader, criterion, device)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if save_path is not None:\n",
    "            checkpoint_file = f\"{save_path}/model_epoch_{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), checkpoint_file)\n",
    "            print(f\"Saved checkpoint: {checkpoint_file}\")\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0792e91",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "def split_train_test(image_list, targets, test_percentage=0.1):\n",
    "    test_percentage = 0.1\n",
    "    num_images = len(image_list)\n",
    "    num_test = int(num_images * test_percentage)\n",
    "\n",
    "    # Randomly sample indices for test set\n",
    "    test_indices = random.sample(range(num_images), num_test)\n",
    "\n",
    "    # Create test images and targets\n",
    "    test_images = [image_list[i] for i in test_indices]\n",
    "    test_targets = [targets[i] for i in test_indices]\n",
    "\n",
    "    # Optionally, remove test items from the training set\n",
    "    train_images = [img for idx, img in enumerate(image_list) if idx not in test_indices]\n",
    "    train_targets = [tgt for idx, tgt in enumerate(targets) if idx not in test_indices]\n",
    "\n",
    "    train_loader = create_dataloader(\n",
    "        train_images, train_targets, processor, device, batch_size=16, shuffle=True\n",
    "    )\n",
    "    test_loader = create_dataloader(\n",
    "        test_images, test_targets, processor, device, batch_size=32, shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5785530",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pretrain_model():\n",
    "    # PRETRAINING: No style head\n",
    "    print(\"=\"*50)\n",
    "    print(\"PRETRAINING MODE (no style head)\")\n",
    "    print(\"=\"*50)\n",
    "    image_list, image_ids = load_image_from_drive()\n",
    "    pretraining_metadata = load_pretraining_metadata()\n",
    "    from augmentation import augment_images_for_pretraining\n",
    "    image_list, image_ids, targets = augment_images_for_pretraining(image_list, image_ids, pretraining_metadata)   \n",
    "\n",
    "\n",
    "    pretrain_model = BLIP2MultiHeadRegression( blip2,\n",
    "        use_style_head=False, train_qformer=False, train_vision=False\n",
    "    )\n",
    "    pretrain_criterion = WeightedMultiHeadLoss( movement_weight=1.0, genre_weight=0.7,\n",
    "        use_style=False,).to(device)\n",
    "    optimizer = torch.optim.AdamW(pretrain_model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    train_loader, test_loader = split_train_test(image_list, targets, test_percentage=0.1)\n",
    "    save_dir = \"./checkpoints\"\n",
    "    history = train_model(pretrain_model, train_loader, test_loader, optimizer, pretrain_criterion,\n",
    "        device, num_epochs=1, save_path=save_dir\n",
    "    )\n",
    "# pretrain_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019ab0a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from transformers import Blip2Processor\n",
    "from augmentation import augment_annotated_images\n",
    "\n",
    "# --- Global variables for lazy loading ---\n",
    "_model, _processor = None, None\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir=\"./checkpoints\"):\n",
    "    \"\"\"Return the latest checkpoint path or None if none exist.\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"model_epoch_*.pt\"))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    # Sort by epoch number\n",
    "    checkpoint_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split(\"_\")[-1]))\n",
    "    return checkpoint_files[-1]\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "BASE_DIR = \"/path/to/your/project\"  # replace with your BASE_DIR\n",
    "\n",
    "def save_model_checkpoint(model):\n",
    "    checkpoint_dir = os.path.join(BASE_DIR, \".checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # Generate abbreviated timestamp (YYMMDD_HHMMSS)\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "    # Build checkpoint path\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_{timestamp}.pt\")\n",
    "\n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "def initialize_model_for_webaccess():\n",
    "    \"\"\"\n",
    "    Initialize the BLIP2 multi-head regression model and processor.\n",
    "    Loads the latest checkpoint if available.\n",
    "    \"\"\"\n",
    "    model = BLIP2MultiHeadRegression(\n",
    "        blip2,\n",
    "        use_style_head=True,\n",
    "        train_qformer=True,\n",
    "        train_vision=False\n",
    "    )\n",
    "\n",
    "    latest_ckpt = get_latest_checkpoint()\n",
    "    if latest_ckpt is not None:\n",
    "        model.load_state_dict(torch.load(latest_ckpt, map_location=\"cpu\"))\n",
    "        print(f\"Loaded model weights from {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found, using untrained weights.\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    processor = Blip2Processor.from_pretrained(local_model_path, use_fast=True)\n",
    "    return model, processor\n",
    "\n",
    "def get_model_and_processor():\n",
    "    \"\"\"\n",
    "    Lazy-load the model and processor.\n",
    "    \"\"\"\n",
    "    global _model, _processor\n",
    "    if _model is None or _processor is None:\n",
    "        _model, _processor = initialize_model_for_webaccess()\n",
    "        print(f\"Model and processor ready\")\n",
    "    return _model, _processor\n",
    "\n",
    "def forward_images(images):\n",
    "    model, processor = get_model_and_processor()\n",
    "    model.eval()\n",
    "\n",
    "    # Process all images as a batch\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    embeddings = outputs[\"combined\"].cpu().tolist()\n",
    "    print(f\"Forward pass completed on {len(images)} images\")\n",
    "    return embeddings\n",
    "\n",
    "def backward_single_image(image, target, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Perform a single training step on one image.\n",
    "    \"\"\"\n",
    "    model, processor = get_model_and_processor()\n",
    "    criterion = WeightedMultiHeadLoss(movement_weight=1.0, genre_weight=1.0, use_style=True).to(device)\n",
    "\n",
    "    augmented_images, augmented_targets = augment_annotated_images([image], [target])\n",
    "    print(f\"Augmented to {len(augmented_images)} images for training\")\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "    inputs = processor(images=augmented_images, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    target_tensor = torch.tensor(augmented_targets, dtype=torch.float32).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    print(\"Model outputs obtained\", outputs.keys())\n",
    "    loss, loss_dict = criterion(outputs, target_tensor)\n",
    "\n",
    "    print(\"Backward pass with loss:\", loss.item(), loss_dict)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    return loss.item(), loss_dict"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (my_venv)",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
